{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中的深度神经网络\n",
    "### 代码\n",
    "#### TensorFlow MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('.', one_hot = True, reshape = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用TensorFlow提供的MNIST数据集，它把分批和独热码都处理好了。  \n",
    "### 学习参数 Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "## 参数 Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128 # 如果没有足够内存，可以降低batch size\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784 # MNIST data input(img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes(0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐藏层参数 Hidden Layer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layer = 256 # 特征的层数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_hidden_layer 决定了神经网络隐藏层的大小，也被称作层的宽度。   \n",
    "### 权重和偏置项 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight and bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input,n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深层神经网络有多个层，每个层有自己的权重和偏置项。'hidden_layer'的权重和偏置项只属于隐藏层，'out'的权重和偏置项只属于输出层。如果神经网络比这更深，那每一层都有权重和偏置项。\n",
    "### 输入 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder('float',[None,28,28,1])\n",
    "y = tf.placeholder('float',[None, n_classes])   \n",
    "\n",
    "x_flat = tf.reshape(x,[-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 数据集是由 28px *28px单通道图片组成。tf.reshape()函数把 28px *28px的矩阵转换为784px*1px的单行向量x。\n",
    "### 多层感知器 Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ReLU 作为隐藏层激活函数\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "#输出层的线性激活函数\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']),biases['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.add(tf.matmul(x_flat, weights['hidden_layer']),biases['hidden_layer']),就是xW+b。把线性函数与ReLU组合在一起，形成一个2层网络。\n",
    "### 优化器 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义误差值和优化器\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, \n",
    "                                                              labels = y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 训练循环\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # 遍历所有batch\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # 运行优化器进行反向传导，计算cost（获取loss的值）\n",
    "            sess.run(optimizer, feed_dict = {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow中的MNIST库提供了分批接收数据的能力。调用mnist.train.next_batch()函数返回训练数据的一个子集。  \n",
    "## 保存和读取 TensorFlow 模型  \n",
    "训练一个模型的时间很长。但是你一旦关闭了TensorFlow session, 你所有训练的权重和偏置项都丢失了。如果你计划在之后重新使用这个模型，你需要重新训练！  \n",
    "幸运的是， TensorFlow 可以让你通过一个叫 tf.train.Saver的类把你的进程保存下来，这个累可以把任何 tf.Variable 存到你的文件系统。  \n",
    "### 保存变量  \n",
    "让我们通过一个简单的例子来保存 weights和 bias Tensors。 第一个例子你只是存两个变量，后面会教你如何把一个实际模型的所有权重保存下来。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "[[-0.35832396 -1.99825037  0.14570355]\n",
      " [-1.41003752 -1.92391455 -1.036587  ]]\n",
      "Bias:\n",
      "[ 0.12552246 -1.36862433  0.75178486]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# 文件保存路径\n",
    "save_file = '.model/ckpt'\n",
    "\n",
    "# 两个Tensor 变量： 权重和偏置项\n",
    "weights = tf.Variable(tf.truncated_normal([2,3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# 用来存取 Tensor 变量的类\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 初始化所有变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 显示变量和权重\n",
    "    print('weights:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "    \n",
    "    # 保存模型\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights 和 bias Tensors 用tf.truncated_normal()函数设定了随机值。用tf.train.Saver.save()函数把这些值被保存在save_file位置，命名为'model.ckpt',('.ckpt'扩展名表示'checkpoint')。  \n",
    "## 加载变量  \n",
    "现在这些变量已经存好了，让我们把它们加载到新模型里。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .model/ckpt\n",
      "weight:\n",
      "[[ 0.15636076 -0.66142511 -0.45238689]\n",
      " [-0.92793959 -0.45580754  0.11625437]]\n",
      "bias:\n",
      "[-0.28037843  0.49187064 -0.09098915]\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "# 移除之前的权重和偏置项  \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 两个变量： 权重和偏置项  \n",
    "weights = tf.Variable(tf.truncated_normal([2,3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# 用来存取Tensor变量的类\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 加载权重和偏置项\n",
    "    saver.restore(sess, save_file)\n",
    "    \n",
    "    # 显示权重和偏置项\n",
    "    print('weight:')\n",
    "    print(sess.run(weights))\n",
    "    print('bias:')\n",
    "    print(sess.run(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存一个训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('.', one_hot = True)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# weights and bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# logits - xW+b\n",
    "logits = tf.add(tf.matmul(features, weights),bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits, labels = labels ))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# calcuate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 让我们训练模型并保存权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "# 启动图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tflearn]",
   "language": "python",
   "name": "conda-env-tflearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
