{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习(Reinforcement Learning)基础知识 \n",
    "### 预备知识\n",
    "> **强化学习**  \n",
    "\n",
    "强化学习通常包括两个实体agent和environment.两个实体的交互如下：在environment的state $s_t$下，agent采取action $a_t$进而得到reward $r_t$并进入state $s_{t+1}$.  \n",
    "强化学习的问题，通常有如下特点：\n",
    "- 不同的action产生不同的reward  \n",
    "- reward有延迟性  \n",
    "- 对某个action的reward是基于当前的state的  \n",
    "\n",
    ">** 马尔可夫过程**  \n",
    "\n",
    "状态遵循马尔可夫是指：  \n",
    "$$P[S_{t+1}|S_t] = P[S_{t+1}|S_t,\\cdots,S_1]$$\n",
    "即未来与过去无关，只与现在的状态有关。  \n",
    "$<S,P>$是马尔可夫过程是指S为有限状态集合并且遵循马尔可夫，P是状态转移概率矩阵：  \n",
    "$$P_{s,s'}=P[S_{t+1}=s'|S_t=s]$$\n",
    "\n",
    "\n",
    "### 1. 马尔可夫决策过程(Markov Decision Processes, MDPs). \n",
    "MDPs简单说就是一个智能体(Agent)采取行动(Action)从而改变自己的状态(State)获得奖励(Reward)与环境(Environment)发生交互的循环过程。  \n",
    "MDP的策略完全取决于当前的状态(Only present matters),这也是它马尔科夫性的体现。  \n",
    "简单表示为： $M = <S,A,P_{s,a},R>$  \n",
    "\n",
    "**强化学习**  \n",
    "\n",
    "强化学习通常包括两个实体agent和environment.两个实体的交互如下：在environment的state $s_t$下，agent采取action $a_t$进而得到reward $r_t$并进入state $s_{t+1}$.\n",
    "### 基本概念  \n",
    "1. $s\\in S$: 有限状态state的集合，s表示某个特定状态。 \n",
    "2. $a\\in A$: 有限动作action的集合，a表示某个特定动作。 \n",
    "3. Transition Model $T(S,a,S')\\sim P_r(s'|s,a)$: 根据当前状态 s 和动作 a 预测下一个状态 s'，这里的$P_r$表示从 s 采取行动 a 转移到 s' 的概率。P为三维概率矩阵。    \n",
    "4. Reward 回报函数： $ R(s,a) = E[R_{t+1}|s,a]$: 表示 agent采取某个行动后的即时奖励，\n",
    "它还有R(s,a,s')等表现形式，采用不同形式，其意义略有不同。  \n",
    "有时R与A无关，$ R_s = E[R_{t+1}|s]$  \n",
    "奖励函数$R_{t+1}$定义为从状态 $S_{t}$ 到 $S_{t+1}$所得到的奖励。\n",
    "5. Policy  $\\pi(s)\\rightarrow a$:根据当前state来产生action,可表现为 $a = \\pi(s)$或者$\\pi(a|s) = P[a|s]$,后者表示某种状态下执行某个动作的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回报(Return):  \n",
    "$U(s_0s_1s_2\\cdots)$与折扣率(discount)  $\\gamma\\in[0,1]$:U代表执行一组action后所有状态累计的reward之和，但由于直接的reward相加在无限时间序列中会导致无偏向，而且会产生状态的无限循环。因此在这个Utility函数中引入 $\\gamma$折扣率这一概念，令往后的状态所反馈回来的reward乘上这个系数，这样意味着当下的reward比未来反馈的reward更重要，这也比较符合直觉。定义：  \n",
    "\n",
    "$$U(s_0s_1s_2\\cdots) = \\sum_{t=0}^{\\infty} \\gamma^tR(s_t) \\le \\sum_{t=0}^{\\infty} \\gamma^tR_{max} = \\dfrac{R_{max}}{1-\\gamma},\\qquad (0<\\gamma<1)$$  \n",
    "由于我们引入了discount,($\\gamma$表示学习随着时间推移的折扣率)，可以看到我们把一个无限长度的问题转换成了一个拥有最大值上限的问题。  \n",
    "\n",
    "**强化学习的目的是最大化长期未来奖励，即寻找最大的U.**  \n",
    "\n",
    "基于回报，我们再引入两个函数：  \n",
    "- 状态价值函数: $v(s) = E[U_t|S_t = s]$, 意义为基于t时刻的状态s能获得的未来回报return的期望，加入动作选择策略后可表示为$v_\\pi(s)=E_\\pi[U_t|S_t = s]$  \n",
    ">  奖励函数$R_{t+1}$定义为从状态 $S_{t}$ 到 $S_{t+1}$所得到的奖励。那么时刻0所能得到的回报可以写为：\n",
    "$U_0 = R_1+\\gamma R_2+\\gamma^2 R_3+\\cdots$  \n",
    "> t时刻在某一状态下的回报为：\n",
    "$U_t=R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{T-t-1}R_T+\\cdots$\n",
    "- 动作价值函数： $q_\\pi = E_\\pi[U_t|S_t=s, A_t=a]$, 意义为基于t时刻的状态s，选择一个action后能获得的未来回报的期望。  \n",
    "\n",
    "价值函数用来衡量某一状态或动作-状态的优劣，即对智能体来说是否值得选择某一状态或在某一状态下执行某一动作。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP求解  \n",
    "我们需要找到最优的策略来使未来回报最大化，求解过程大致可分为两步：  \n",
    "1. 预测：给定策略，评估相应的状态价值函数和状态-动作价值函数。\n",
    "2. 行动：根据价值函数得到当前状态对应的最优动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman期望方程\n",
    "> 1. 一般的状态价值函数\n",
    "\\begin{eqnarray}\n",
    "v(s)&=&E[U_t|S_t=s]\\notag\\\\\n",
    "&=& E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\cdots|S_t=s]\\notag\\\\\n",
    "&=& E[R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\cdots)|S_t=s]\\notag\\\\\n",
    "&=& E[R_{t+1}+\\gamma v(S_{t+1})|S_t = s]\\notag\\\\\n",
    "&=& R_s+\\gamma \\sum_{s'\\in S} P_{s,s'} v(s')\\notag\n",
    "\\end{eqnarray}  \n",
    "上式就是Bellman方程的基本形态，得到线性方程组:  \n",
    "$$ v = R +\\gamma Pv$$可求得每个状态的价值。 \n",
    "由矩阵形式的Bellman方程，可直接求出：\n",
    "$$v = (I-\\gamma P)^{-1}R$$,其复杂度为$O(n^3)$,一般可通过动态规划、蒙特卡洛估计求解。  \n",
    "\n",
    "> 2.加入**动作选择策略**后，状态价值函数公式变为：  \n",
    "$$v_\\pi(s) = E_\\pi[U_t|S_t=s] =\\sum_{a\\in A}\\pi(a|s)\\left(R_s^a+\\gamma \\sum_{s'\\in S} P_{s,s'}^a v_\\pi(s')\\right) $$  \n",
    "其中，$P_{s,s'}^a = P[S_{t+1}=s'|S_t=s,A_t=a]$，  \n",
    "$P_{s,s'}^\\pi = \\sum_{a\\in A}\\pi(a|s)P_{s,s'}^a$,  \n",
    "$ R_s^\\pi = \\sum_{a\\in A} \\pi(a|s)R_s^a$,  \n",
    "上式中，策略$\\pi$是指在给定状态s的情况下，动作a的概率分布，即$\\pi(a|s)=P(a|s)$.  \n",
    "\n",
    "> 3.我们将和转换为期望，上式等价于： \n",
    "$$v_\\pi(s) =  E_\\pi[R_s^a+\\gamma v_\\pi(S_{t+1})|S_t = s]$$ \n",
    "同理，我们可以得到动作价值函数的公式如下：  \n",
    "$$ q_\\pi(s,a) = E_\\pi[R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1})|S_t=s, A_t=a]$$  \n",
    "> 4.状态价值函数和动作价值函数的关系 \n",
    "\n",
    "$$v_\\pi(s)=\\sum_{a\\in A} \\pi(a|s)q_\\pi(s,a) = E[q_\\pi(s,a)|S_t=s]$$  \n",
    "$$q_\\pi(s,a) = R_s^a +\\gamma\\sum_{s'\\in S}P_{s,s'}^av_\\pi(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最优方程  \n",
    "#### 1.最优价值函数和最优动作价值函数\n",
    "定义最优价值函数$v^*: S\\rightarrow R$\n",
    "$$v^*(s) = \\max_{\\pi} v_\\pi(s)$$  \n",
    "定义最优动作价值函数$q^*: S\\rightarrow R$  \n",
    "$$q^*(s,a) = \\max_{\\pi} q_\\pi(s,a)$$  \n",
    "其意义为所有策略下价值函数的最大值。  \n",
    "- 策略的偏序关系  \n",
    "$$\\pi'\\ge\\pi\\Leftrightarrow v_\\pi'(s)\\ge v_\\pi(s),\\forall s\\in S$$  \n",
    "\n",
    "**定理**  \n",
    "对于任意一个MDP:\n",
    "> - 存在一个最优策略$\\pi^*$使得对于$\\forall \\pi, \\pi^*\\ge\\pi$\n",
    "> - 所有的最优策略对应的价值函数就是最优价值函数\n",
    "$$v_{\\pi^*}(s) = v^*(s)$$ \n",
    "> - 所有的最优策略对应的动作价值函数就是最优动作价值函数\n",
    "$$ q_{\\pi^*}(s,a) = q^*(s,a)$$  \n",
    "\n",
    "根据这个定理，可以得到Bellman最优方程：  \n",
    "$$v^*(s) = \\max_{a}q^*(s,a)$$  \n",
    "$$q^*(s,a)= R_s^a +\\gamma\\sum_{s'\\in S} P_{s,s'}^a v^*(s')$$  \n",
    "- v 描述了处于一个状态的长期最优化价值，即在这个状态下考虑到所有可能发生的后续动作，并且都挑选最优的动作来执行的情况下，这个状态的价值。  \n",
    "- q 描述了处于一个状态并执行某个动作后所带来的长期最优价值，即在这个状态下执行某一特定动作后，考虑再之后所有可能处于的状态并且在这些状态下总是选取最优动作来执行所带来的长期价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Q-learning  \n",
    "Q-learning算法的转移规则比较简单，如下式所示：  \n",
    "$$Q(s,a) = R(s,a) +\\gamma\\cdot \\max_{\\tilde{a}}{\\{Q(\\tilde{s},\\tilde{a})}\\}\\qquad (*)$$  \n",
    "其中，s, a表示当前的状态和行为，$\\tilde{s},\\tilde{a}$表示 s的下一个状态和行为，学习参数$\\gamma$为满足$0\\le\\gamma<1$的常数。  \n",
    "在没有老师的情况下，我们的agent将通过经验进行学习(也称为无监督学习)。它不断从一个状态转至另一状态进行探索，直至到达目标。我们将agent的每一次探索称为一个episode.在每一个episode中，agent从任意初始状态到达目标状态。当agent达到目标状态后，一个episode结束，紧接着进入另一个episode。  \n",
    "\n",
    "**下面给出整个Q-learning算法的计算步骤**  \n",
    "- 算法1.1 (Q-learning算法)  \n",
    "> Step 1 给定参数 $\\gamma$ 和 reward 矩阵R.  \n",
    "> Step 2 令Q := 0.  \n",
    "> Step 3 For each episode:  \n",
    ">> 3.1 随机选择一个初始的状态 s  \n",
    ">> 3.2 若未达到目标状态，则执行以下几步。\n",
    ">>> (1)在当前状态s的所有可能行为中选取一个行为 a.  \n",
    ">>> (2)利用选定的行为a,得到下一个状态$\\tilde{s}$.  \n",
    ">>> (3)按照 (*) 式计算Q (s,a).  \n",
    ">>> (4)令$s := \\tilde{s}$.  \n",
    "\n",
    "Agent利用上述算法从经验中进行学习。每一个episode相当于一个training session. 在一个training session中， agent探索外界环境，并接受外界环境的reward, 直到达到目标状态。训练的目标是要强化agent的大脑(Q)。训练得越多，则Q被优化得更好。当矩阵Q被训练强化后，agent便更容易找到达到目标状态的最快路径了。  \n",
    " \n",
    "公式(*)中的$\\gamma$满足$0\\le\\gamma<1$.$\\gamma$趋向0表示agent主要考虑immediate reward, 趋向1表示agent将同时考虑future rewards.  \n",
    "利用训练好的矩阵Q，我们可以很容易地找出一条从任意状态$s_0$出发达到目标状态的行为路径，具体步骤如下：  \n",
    "1. 令当前状态$ s:=s_0$.  \n",
    "2. 确定a， 它满足$Q(s,a) = \\max_{\\tilde{a}} {Q(s,\\tilde{a}})$.  \n",
    "3. 令当前状态 $s: \\tilde{s} (\\tilde{s})$表示a对应的下一个状态）.  \n",
    "4. 重复执行步2和步3知道 s成为目标状态。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
