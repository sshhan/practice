{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现反向传播的具体操作\n",
    "现在我们只考虑一个简单的神经网络，它只有一个隐藏层和一个输出节点。这是通过反向传播\n",
    "更新权重的算法概述：  \n",
    "- 把每一层权重更新的初始步长设置为0\n",
    "> - 输入到隐藏层的权重更新是$\\Delta w_{ij}=0$\n",
    "> - 隐藏层到输出层的权重更新是$\\Delta W_j = 0$\n",
    "  \n",
    "  \n",
    "- 对训练数据当中的每一个点\n",
    "> - 让它正向通过网络，计算输出$\\hat{y}$\n",
    "> - 计算输出节点的误差梯度 $\\delta^o =(y-\\hat{y})\\hat{y}(1-\\hat{y})$,这里\n",
    "$\\hat{y}=f(z),z=\\sum_j W_j bj$是输出节点的输入。\n",
    "> - 误差传播到隐藏层 $\\delta_j^h=\\delta^o W_j b_j(1-b_j)$,这里$b_j=f(\\sum_i w_{ij}x_i)$\n",
    "> - 更新权重步长：\n",
    "> >  $\\Delta W_j=\\Delta W_j +\\delta^0 b_j$  \n",
    "> >  $\\Delta w_{ij}=\\Delta w_{ij}+\\delta_j^h x_i$  \n",
    "- 更新权重，其中 $\\eta$是学习率，m是数据点的数量：  \n",
    "> - $W_j=W_j+\\eta\\Delta W_j/m$\n",
    "> - $w_{ij}=w_{ij}+\\eta w_{ij}/m$\n",
    "- 重复这个过程e代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意：\n",
    "首先你需要初始化权重。我们希望它们比较小，这样输入在 sigmoid 函数那里可以在接近 0 的位置，而不是最高或者最低处。  \n",
    "很重要的一点是要随机地初始化它们，这样它们有不同的初始值，是发散且不对称的。  \n",
    "所以我们用一个中心为 0 的正态分布来初始化权重，此正态分布的标准差（scale 参数）最好使用 $1/\\sqrt{n}$,\n",
    "其中n是输入单元的个数。这样就算是输入单元的数量变多，sigmoid 的输入还能保持比较小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "n_hidden=2\n",
    "epochs=900\n",
    "learnrate=0.005\n",
    "\n",
    "n_records, n_features= features.shape\n",
    "last_loss=None\n",
    "# Initialize weights\n",
    "weights_input_hidden=np.random.normal(scale=1/(n_features**0.5),\n",
    "                                      size=(n_features,n_hidden))\n",
    "weights_hidden_output=np.random.normal(scale=1/(n_features**0.5),\n",
    "                                      size=n_hidden)\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden=np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output=np.zeros(weights_hidden_output.shape)\n",
    "    \n",
    "    for x,y in zip(features,targets):\n",
    "        # calculate output\n",
    "        hidden_layer_input=np.dot(x,weights_input_hidden)\n",
    "        hidden_layer_output=sigmoid(hidden_layer_input)\n",
    "        \n",
    "        output_layer_in=np.dot(hidden_layer_output,weights_hidden_output)\n",
    "        output=sigmoid(output_layer_in)\n",
    "        \n",
    "        # calculate error term for the output unit\n",
    "        \n",
    "        error=y-output\n",
    "        output_error_term=error*output*(1-output)\n",
    "        \n",
    "        #propagate errors to hidden layer\n",
    "        \n",
    "        # calculate the error term for the hidden layer\n",
    "        hidden_error=output_error_term*weights_hidden_output\n",
    "        hidden_error_term=hidden_error*hidden_layer_output*(1-hidden_layer_output)\n",
    "        # update the change in weights\n",
    "        del_w_input_hidden += hidden_error_term*x[:,None]\n",
    "        del_w_hidden_output += output_error_term*hidden_layer_output\n",
    "        #update weights\n",
    "    weights_input_hidden += learnrate*del_w_input_hidden/n_records\n",
    "    weights_hidden_output += learnrate*del_w_hidden_output/n_records\n",
    "        \n",
    "        #print out mean square error on training set\n",
    "    if e%(epochs/10) == 0:\n",
    "        hidden_output=sigmoid(np.dot(x,weights_input_to_hidden))\n",
    "        out=sigmoid(np.dot(hidden_output,weights_hidden_to_output))\n",
    "            \n",
    "        loss=np.mean((out-targets)**2)\n",
    "            \n",
    "        if last_loss and last_loss < loss:\n",
    "            print('Train loss:', loss,'Warning-Loss Increasing')\n",
    "        else:\n",
    "            print('Train loss:',loss)\n",
    "        last_loss = loss\n",
    "#calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(feature_test, weights_input_hidden))\n",
    "out= sigmoid(np.dot(hidden,weights_hidden_output))\n",
    "predictions = out >0.5\n",
    "accuracy=np.mean(predictions == targets_test) \n",
    "print('Predictions accuracy: {:.3f}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
