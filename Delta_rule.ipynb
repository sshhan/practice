{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习中的LMS算法($\\delta$规则)\n",
    "BP算法实质是LMS(Least Mean Square)算法的推广。LMS试图使网络的均方误差最小化，可用于神经元激活函数可微的感知机学习；将LMS算法推广到由非线性可为神经元组成的多层前馈网络就得到BP算法，因此BP算法亦称广义$\\delta$规则。  \n",
    "因此，在学习BP算法之前，我们有必要学习LMS算法。\n",
    "## 1.简介\n",
    "LMS亦称Widrow-Hoff规则或者$\\delta$规则。  \n",
    "在机器学习，$\\delta$规则是一种梯度下降学习规则-用于在一个单层的神经网络中更新输入的权重到人工神经元。这是更一般的反向传播算法的特例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设激活函数为$g(x)$,对于神经元j的第i个权重$w_{ji}$在$\\delta$规则下被定义为：    \n",
    "\n",
    "$$\\Delta w_{ji}=\\alpha(y_j-\\hat{y}_j)g'(h_j)x_i$$  \n",
    "这里,\n",
    "> - $\\alpha$是常量，称为学习速率；  \n",
    " $g(x)$是神经元激活函数；  \n",
    " $y_j$是目标输出；  \n",
    " $h_j$是神经元j的输入值的加权和,$h_j=\\sum_i x_iw_{ji}$;    \n",
    " $\\hat{y}_j$是实际输出,$\\hat{y}_j=g(h_j)$;  \n",
    " $x_i$是第i个输入值\n",
    "  \n",
    "对于具有线性激活函数的神经元，$\\delta$规则通常以简化形式陈述:   \n",
    "\n",
    "$$\\Delta w_{ji}=\\alpha(y_j-\\hat{y}_j)x_i$$   \n",
    "- 虽然德尔塔规则类似于感知器的更新规则，但推导是不同的。感知器使用阶跃函数(Heaviside step function)作为激活函数这意味着$g'(x)$在0点处不存在，在其他地方处处为0，这使得不可能直接应用$\\delta$规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. $\\delta$规则的推导。\n",
    "$\\delta$规则是通过尝试通过梯度下降来最小化神经网络的输出的误差来导出的。一个神经网络的输出误差由下式度量：  \n",
    "$$E=\\dfrac{1}{2}\\sum_j(y_j-\\hat{y}_j)^2$$  \n",
    "最小化误差：我们计算误差函数相对于每个权重的偏导数。这个导数计算如下：  \n",
    "$$\\dfrac{\\partial E}{\\partial w_{ji}}=-(y_j-\\hat{y}_j)g'(h_j)x_i$$  \n",
    "如上所述，梯度下降告诉我们，每个权重的变化应该与梯度的变化成比例。  \n",
    "选择一个比例常数$\\alpha$并消除负号，使我们能够在梯度的负方向上移动权重从而最小化误差，目标方程如下：  \n",
    "$$\\Delta w_{ji}=\\alpha(y_j-\\hat{y}_j)g'(h_j)x_i$$  \n",
    "## 3. 应用\n",
    "对于存在隐藏层的多层感知器，必须使用更复杂的算法，如反向传播。如果函数是非线性的且可微分的，则可以使用诸如$\\delta$规则的方法。  \n",
    "当多个感知器结合在人工神经网络中时，每个输出神经元独立于所有其他神经元运行; 因此，学习每个输出可以被孤立地考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.LMS算法中学习率的选择\n",
    "在线性神经网络中，学习率参数$\\eta$的选择非常重要，直接影响了神经网络的性能和收敛性。  \n",
    "### 4.1 确保网络稳定收敛的学习率\n",
    "$\\eta$越小，算法运行的时间就越长，算法也就记忆了更多过去的数据。因此，$\\eta$的倒数反映了LMS算法的记忆容量大小。  \n",
    "1996年Hayjin证明，只要学习率$\\eta$满足下式，LMS算法就是按方差收敛的：\n",
    "$$0<\\eta<\\dfrac{2}{\\lambda_{max}}$$  \n",
    "其中，$\\lambda_{max}$是输入向量x组成的自相关矩阵R的最大特征值，由于$\\lambda_{max}$常常不可知，因此往往使用自相关矩阵R的迹(trace)来代替。按定义，矩阵的迹是矩阵主对角线元素之和：  \n",
    "$$tr(R)=\\sum_{i=1}^nR(i,i)$$  \n",
    "同时，矩阵的迹又等于矩阵所有特征值之和，因此一般有$\\mathrm{tr}(R)>\\lambda_{max}$,只要取：  \n",
    "$$0<\\eta<\\dfrac{2}{\\mathrm{tr}(R)}<\\dfrac{2}{\\lambda_{max}}$$  \n",
    "即可满足条件。按定义，自相关矩阵的主对角线元素就是各输入向量的均方值。因此，公式又可写成:\n",
    "$$0<\\eta<\\dfrac{2}{\\mathrm{向量均方值之和}}$$  \n",
    "### 4.2 学习率逐渐下降\n",
    "在感知机学习算法中曾提到，学习率$\\eta$随着学习的进行逐渐下降比始终不变更加合理。在学习的初期，用比较大的学习率保证收敛速度，随着迭代次数增加，减小学习率以保证精度，确保收敛。\n",
    "- 一种可能的学习率下降方案是：  \n",
    "$$\\eta=\\dfrac{\\eta_0}{m}$$  \n",
    "这里m是迭代次数。  \n",
    "在这种学习方法中，学习率会随着迭代次数的增加较快下降。  \n",
    "- 另一种方法是指数式下降：  \n",
    "$$\\eta=c^m\\eta_0$$  \n",
    "c是一个接近1但小于1的常数。Darken与Moody于1992年提出搜索一收敛方案，计算公式如下：  \n",
    "$$\\eta=\\dfrac{\\eta_0}{1+\\dfrac{m}{\\tau}}$$  \n",
    "$\\eta_0$和$\\tau$均为常量。当迭代次数较小时，学习率$\\eta=\\eta+0$,随着迭代次数增加，学习率逐渐下降，公式近似于:  \n",
    "$$\\eta=\\dfrac{\\eta_0}{0+\\dfrac{m}{\\tau}}=\\dfrac{\\tau\\eta_0}{n}$$  \n",
    "  \n",
    "LMS算法的一个缺点是，它对输入向量自相关矩阵R的条件数敏感。当一个矩阵的条件数比较大时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
