{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "hello_constant = tf.constant('Hello World!')\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "在Tensorflow 中，数据不是以整数、浮点数或者字符串形式存储的。这些值被封装在一个叫做tensor的对象中。在hello_constant = tf.constant('Hello World!')代码中，hello_constant是一个0维度的字符串tensor,tensor还有很多不同大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = tf.constant(12334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = tf.constant([123,456,789])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant是经常使用的TensorFlow运算之一。tf.constant()返回的tensor是一个常量tensor,因为这个tensor的值不会变。\n",
    "## Session\n",
    "TensorFlow的api构建在computational graph的概念上，它是一种对数学运算过程进行可视化的方法。一个'TensorFlow Session'是用来运行图的环境。这个session负责匹配GPU(s)和(或)CPU(s),包括远程计算机的运算。使用方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码用tf.Session创建了一个sess的Session实例。然后，sess.run()函数对tensor求值，并返回结果。 \n",
    "## 输入\n",
    "如果你想使用一个非常量(non-constant)，这就是tf.placeholder()和feed_dict派上用场的时候了。\n",
    "### tf.placeholder()\n",
    "很遗憾，你不能把数据集赋值给x再将它传给TensorFlow.因为之后你会想要你的TensorFlow模型对不同的数据集采用不同的参数。你需要的是tf.placehoder()!  \n",
    "数据经过tf.session.run()函数得到的值，由tf.placeholder()返回成一个tensor,这样你可以在session运行之前，设置输入  \n",
    "### Session的feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict = {x: 'Hello World'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用tf.session.run()里的feed_dict参数，设置占位tensor.上面的例子显示tensor x被设置成字符串\"Hello, world\".如下所示，也可以用feed_dict设置多个tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict = {x: 'Test String', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意：  \n",
    "如果传人feed_dict的数据与tensor类型不符，就无法被正确处理，你会得到“ValueError: invalid literal for...”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow数学\n",
    "获取输入很棒，但是现在你需要使用它。你将使用每个人都懂的基础数学运算，加减乘除，来处理Tensor.\n",
    "### 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(5,2)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从加法开始，tf.add()函数如你所想，它传入两个数字、两个tensor、或数字和tensor各一个，以tensor的形式返回它们的和。\n",
    "### 减法和乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.subtract(10, 4)\n",
    "z = tf.multiply(2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y tensor求值结果是 6，因为10 - 4 = 6。z tensor求值结果是10，因为2*5 = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类型转换  \n",
    "为了让特定运算能运行，有时对类型进行转换。例如，你尝试以下代码，会报错：  \n",
    "**tf.subtract(tf.constant(2.0),tf.constant(1))**  \n",
    "这是因为常量1是整数，但是常量2.0是浮点数，subtract需要它们的类型匹配。  \n",
    "在这种情况下，你可以确保数据都是同一类型,或者强制转换一个值为另一个类型。这里，我们可以把2.0转换成整数再相减，这样就能得出正确的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结：  \n",
    "通过上面的学习，我们完成了：  \n",
    "- 在tf.Session里面进行运算。\n",
    "- 通过tf.constant()创建常量tensor  \n",
    "- 用tf.placeholder()和feed_dict得到输入  \n",
    "- 应用tf.add(), tf.subtract(), tf.multiply()和tf.divide()函数进行数学运算。\n",
    "- 学习如何使用tf.cast()进行类型转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow里的线性函数\n",
    "神经网络中最常见的运算，就是计算输入、权重和偏差的线性组合。我们把线性运算的输出写成：  \n",
    "$$y = xW + b $$  \n",
    "这里W是连接两层的权重矩阵。输出y，输入x，偏差b全都是向量。  \n",
    "### TensorFlow里的权重和偏差\n",
    "训练神经网络的目的是更新权重和偏差来更好地预测目标。为了使用权重和偏差，你需要一个能修改的tensor,这就排除了tf.placeholder()和tf.constant()，因为它们的Tensor不能改变。这里就需要tf.Variable了。\n",
    "### tf.Variable( )  \n",
    "tf.Variable类创建一个tensor, 其初始值可以被改变，就像普通的python变量一样。该tensor把它的状态存在session里，所以你必须手动初始化它的状态。你将使用\n",
    "tf.global_variables_initializer()函数来初始化所有可变tensor。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**初始化**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.global_variables_initializer()会返回一个操作，它会从graph中初始化所有的TensorFlow变量。你可以通过session来调用这个操作来初始化所有上面的变量。\n",
    "用tf.Variable类可以让我们改变权重和偏差，但还是要选择一个初始值。  \n",
    "\n",
    "从正态分布中取随机数来初始化权重是个比较好的做法。随机化权重可以避免模型每次训练时候卡在同一个地方。  \n",
    "\n",
    "类似地，从正态分布中选择权重可以避免任意一个权重与其他权重相比有压倒性的特性。可以用\n",
    "tf.truncated_normal()函数从一个正态分布中生成随机数。  \n",
    "### tf.truncated_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.truncated_normal()返回一个tensor,它的随机值取自一个正态分布，并且它们的取值会在这个正态分布均值的两个标准差之内。  \n",
    "因为权重已经被随机化来帮助模型不被卡住，你不需要再把偏差随机化了。让我们简单地偏差设为0.\n",
    "### tf.zeros()\n",
    "返回一个都是0的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Softmax\n",
    "Softmax函数可以把它的输入，通常被称为logits或者logit scores, 处理成0到1之间，并且能够把输出归一化到和为1。这意味着softmax函数与分类的概率分布等价。它是一个网络预测多分类问题的最佳输出激活函数。   \n",
    "\n",
    "\n",
    "当我们用TensorFlow来构建一个神经网络时，相应地，它有一个计算softmax的函数。\n",
    "tf.nn.softmax()直接实现了softmax函数，它输入logits,返回softmax激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.nn.softmax([2.0,1.0,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "在下面使用softmax函数返回logits的softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Scikit-Learn 实现 One-Hot Encoding\n",
    "scikit-learn的LabelBinarizer函数可以很方便地把你的目标(labels)转化成独热编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# create the encoder创建编码器\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# 编码器找到类别并分配one-hot向量\n",
    "lb.fit(labels)\n",
    "\n",
    "#最后把目标转换成独热编码向量\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow中的交叉熵(Cross Entropy)  \n",
    "\n",
    "$$ D(\\hat{y}-y) = -\\sum_j y_j\\ln \\hat{y}_j$$\n",
    "创建一个交叉熵函数，需要以下两个新的函数：  \n",
    "- tf.reduce_sum()  \n",
    "- tf.log()\n",
    "\n",
    "### Reduce Sum\n",
    "tf.reduce_sum()函数输入一个序列，返回它们的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.reduce_sum([1,2,3,4,5]) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Log\n",
    "tf.log()返回所输入值的自然对数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "用softmax_data和one_hot_encod_label打印交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = - tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict = {softmax: softmax_data, \n",
    "                                              one_hot: one_hot_data}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化的输入和初始权值  \n",
    "归一化，数据0均值单位方差使得优化更容易进行  \n",
    "- 如果处理的是像素，RGB范围在0-255,则有: $\\dfrac{R-128}{128}, \\dfrac{G-128}{128},\\dfrac{B-128}{128}$， 它不会改变图像的内容，但优化更容易进行。  \n",
    "- 此外，我们希望权重和偏差有一个理想的初始值便于梯度下降，一个简单的方法就是从高斯分布上随机获取初始权值，使其均值为0，标准差为$\\sigma$,$\\sigma$决定了初始化时你的输出的数量级和分布的陡峭程度，大$\\sigma$，说明分布确定性高，小$\\sigma$说明确定性弱，通常初始时取小的$\\sigma$。  \n",
    "- 训练分类器，寻找误差函数的最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching in TensorFlow\n",
    "Mini-batching是一个一次训练数据集的一小部分，而不是整个训练集的技术。它可以使内存较小、不能同时训练整个数据集的电脑也可以训练模型。  \n",
    "Mini-batching从运算角度来说是低效的，因为你不能在所有样本中计算loss.但是这点小代价也比根本不能运行模型要划算。  \n",
    "它和随机梯度下降(SGD)结合在一起也很有帮助。方法是在每一代训练之前，对数据进行随机混洗，然后创建mini-batches, 对每一个mini-batch, 用梯度下降训练网络权重。因为这些batches是随机的，你其实是在对每个batch做随机梯度下降。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑输入、权重和偏置项的总共内存空间需求，可以在CPU和GPU上训练整个数据集。  \n",
    "但将来要用到的数据集可能是以G来衡量，甚至更多。买内存会很贵。所以，需要学会使用mini-batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Mini-Batching  \n",
    "要使用mini-batching,你首先要把你的数据集分成batch。    \n",
    "不幸的是，有时候不可能把数据完全分割成相同数量的batch。例如有1000个数据点，你想每个batch里有128个数据，但是1000无法被128整除。你得到的结果是其中7个batch有128个数据点，一个batch有104个数据点。  \n",
    "batch里面的数据点数量会不同的情况下，你需要利用**TensorFlow**中的tf.placeholder()函数来接收这些不同的batch。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例：\n",
    "如果每个样本有n_input = 784个特征，n_classes =10个可能的标签， features的维度应该是[None, n_input], labels的维度是[None, n_classes]。  \n",
    "\n",
    "features = tf.placeholder(tf.float32,[None,n_input])  \n",
    "labels = tf.placeholder(tf.float32,[None, n_classes])  \n",
    "\n",
    "**None**在这里做什么用呢？  \n",
    "None维度在这里是一个batch size的占位符。在运行时， TensorFlow会接收任何大于0的batch size。  \n",
    "** 问题**：下列参数，会有多少batch,最后一个batch有多少数据点？   \n",
    "features is （50000， 400）  \n",
    "labels is (50000, 10)  \n",
    "batch_size is 128  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 问题：\n",
    "下列参数，会有多少batch,最后一个batch有多少数据点？  \n",
    "features is (50000,400)\n",
    "labels is (50000,10)\n",
    "batch_size = 128  \n",
    "- 总共有多少batch?  \n",
    "  [50000/128] = 391  \n",
    "- 最后一个batch有多少数据点？  \n",
    "  50000-390*128 = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    '''\n",
    "    create batches of features and labels\n",
    "    :param batch_size： The batch size\n",
    "    :param features: list of features\n",
    "    :param labels: list of labels\n",
    "    :return :Batches of (Features, Labels)\n",
    "    '''\n",
    "    assert len(features)==len(labels)\n",
    "    \n",
    "    output_batches = []\n",
    "    sample_size= len(features)\n",
    "    for start_i in range(0,sample_size,batch_size):\n",
    "        end_i= start_i+batch_size\n",
    "        batch = [features[start_i:end_i],labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "## pprint prints data structures like 2d arrays, so they are easier to read\n",
    "\n",
    "pprint(batches(3,example_features,example_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tflearn]",
   "language": "python",
   "name": "conda-env-tflearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
