{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏性理解\n",
    "现在的机器学习问题中，具有非常高维度的数据随处可见。高维度带来的问题不止在于计算量上。在许多生物相关的问题中，数据的维度非常高，但是由于收集数据需要昂贵的实验，因此可用的训练数据相当少，这样的问题通常称为'small n,large p problem'--我们一般用n来表示数据点的个数，p来表示变量的个数，即数据维度。当p>>n的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话，p越大，通常会导致模型越复杂，但是如果n又很小，就会出现严重的过拟合。例如，最简单的线性回归模型：\n",
    "$$f(x)=\\sum_{i=1}^p\\omega_ix_i=\\omega^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用平方损失来进行学习的话，就变成最小化如下的问题：\n",
    "$$J(\\omega)=\\dfrac{1}{2}\\lVert y-Xw \\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里$X=(x_1,\\cdots,x_n)^T\\in R^{n\\times p}$是数据矩阵，而$y=(y_1,\\cdots,y_n)^T$是由标签组成的列向量。该问题具有解析解：\n",
    "$$\\hat{\\omega}=(X^TX)^{-1}X^Ty$$\n",
    "当p>n时，矩阵$X^TX$将不会是满秩的，而这个解也没有办法算出来。或者说是无穷多解。容易过拟合。  \n",
    "解决过拟合最常用的方法就是正则化，著名的岭回归就是添加L2正则子：\n",
    "$$J(\\omega)=\\dfrac{1}{2}\\lVert y-Xw \\rVert_2^2+\\lambda\\lVert \\omega \\rVert_2^2$$\n",
    "直观的来讲，添加这个正则子会使得模型的解偏向于norm比较小的$\\omega$.从凸优化的角度来说，最小化上面这个$J(\\omega)$等价于如下问题：\n",
    "$$\\min\\limits_{\\omega}\\lVert y-Xw \\rVert_2^2, s.t.\\lVert \\omega\\rVert_2^2\\le\\lambda$$\n",
    "我们通过限制$\\omega$的范数的大小实现了对模型空间的限制，从而在一定程度上避免了过拟合，这取决于$\\lambda$的大小。不过岭回归并不具有产生稀疏解的能力，得到的系数$\\omega$仍然需要数据中所有的特征才能计算预测结果，从计算量上来说并未得到改观。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，特别是像在生物或者医学等需要和人交互的领域，稀疏的解除了计算量上的好处之外，最重要的是更具有”可解释性“。比如：一个病如果依赖5个变量的话，将会更易于医生理解、描述和总结规律，但是如果依赖5000个变量的话，基本上就超出人肉可处理的范围了。  \n",
    "在这里引入稀疏性的方法是用L1正则代替L2正则化，得到如下的目标函数：\n",
    "$$J(\\omega)=\\dfrac{1}{2}\\lVert y-Xw \\rVert_2^2+\\lambda\\lVert \\omega \\rVert_1$$\n",
    "该问题通常被称为LASSO(least absolute shrinkage and selector operator).LASSO仍然是一个凸优化问题，不过不再具有解析解。它的优良性质是能产生稀疏性，导致$\\omega$中许多项为0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思考\n",
    "为什么LASSO能够产生稀疏性？\n",
    "首先，和岭回归相似，LASSO问题也等价于如下形式：\n",
    "$$\\min\\limits_{\\omega}\\lVert y-Xw \\rVert_2^2, s.t.\\lVert \\omega\\rVert_1\\le\\lambda$$\n",
    "也就是说，我们将模型空间限制在$\\omega$的一个L1-球中。为了便于可视化，我们考虑两维的情况 ，在$(\\omega_1,\\omega_2)$平面上可以画出目标函数的等高线，而约束条件则称为平米上半径为$\\lambda$的一个球。等高线与范数球首次相交的地方就是最优解。  \n",
    "可以看到，L1-ball和L2-ball的不同就在于它在和每个坐标轴相交的地方都有“角”。而目标函数的测地线除非摆的非常好，大部分时间都会在角的地方相交。注意到在角的位置产生稀疏性，例如图中的相交点就有$\\omega_1=0$.  \n",
    "相比之下，L2-ball就没有这样的性质，因为没有角，所以在第一次相交的地方出现在具有稀疏性的位置的改良就变小了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
