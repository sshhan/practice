{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "## 1.熵\n",
    "**信息熵**的公式：$$H(X)=-\\sum_{i=1}^n p(x_i) \\log p(x_i)$$  \n",
    "其中$p(x_i)$代表随机事件X为$x_i$的概率。  \n",
    "定义为：负对数概率的期望，也就是信息量的数学期望，在信息论中代表随机变量不确定性的度量。  \n",
    "熵越大，随机变量的不确定性就越大。由Jenson不等式可以验证：  \n",
    "$$0\\le H(p)\\le \\log n$$  \n",
    "**条件熵**：  \n",
    "设有随机变量(X,Y)，其联合概率分布为：$$P(X=x_i,Y=y_i)=p_{ij}, i=1,\\cdots,n; j=1,\\cdots,m$$  \n",
    "条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。  \n",
    "随机变量X给定的条件下随机变量Y得到条件熵$H(Y|X)$，定义为Y的条件概率分布的熵对X的数学期望：  \n",
    "$$ H(Y|X)=\\sum_{i=1}^n p_iH(Y|X=x_i)$$  \n",
    "这里，$p_i = P(X=x_i), i=1,\\cdots,n.$\n",
    "### 1.1信息量\n",
    "信息量是对信息的度量，类似于时间的度量是秒。  \n",
    "多少信息用信息量来衡量，我们接受到的信息量跟具体发生的事件有关。  \n",
    "信息的大小跟随机事件的概率有关。越小概率的事件发生了产生的信息量越大，比如汶川地震了；\n",
    "越大概率发生产生的信息量越小，如太阳从东边升起，必然发生的事件。  \n",
    "因此一个具体事件的信息量应该是随着其发生概率而递减的，且不能为负。  \n",
    "$$h(x,y)=h(x)+h(y)$$\n",
    "由于x，y是两个独立发生的事件，则有联合概率分布为：$$p(x,y)=p(x)*p(y)$$\n",
    "根据上面的推导，可以看出h(x)一定与p(x)的对数有关(因为只有对数形式的真数相乘之后，能够对应对数的相加形式)。  \n",
    "因此，我们有信息量的公式如下：\n",
    "$$h(x)=-\\log_2p(x)$$\n",
    "#### 注意：  \n",
    "(1)为什么有一个负号?  \n",
    "负号是为了保证信息量一定是正数或者0  \n",
    "(2)为什么底数为2？  \n",
    "我们只需要信息量满足低概率事件x对应于高的信息量，那么对数的选择是任意的。  \n",
    "若以2为底，此时熵的单位为比特(bit);若以e为底，单位为纳特(nat)。在计算机二进制时，常使用比特熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 信息熵\n",
    "信息论之父克劳德-香农，总结出了信息熵的三条性质：  \n",
    "(1)单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是‘太阳从东方升起’，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。  \n",
    "(2)非负性：即信息熵不能为负。  \n",
    "(3)累加性：即多随机事件同时发生存在的总不确定性的度量是可以表示为各事件不确定性的和。写成公式为：\\\\\n",
    "事件X=A,Y=B同时发生，两个事件独立，则其联合概率分布为：$p(X=A,Y=B)=p(X=A)\\times p(Y=B)$\\\\\n",
    "那么信息熵$H(A,B)=H(A)+H(B)$  \n",
    "香农从数学上，严格证明了满足上述三个条件的随机变量不确定性度量函数具有唯一形式：\n",
    "$$H(X)=-C\\sum_{x}p(x)\\log p(x)$$  \n",
    "其中的C为常数，我们将其归一化为C=1即得到信息熵公式。  \n",
    "补充一下，如果两个事件不相互独立，那么满足：  \n",
    "H(A,B)=H(A)+H(B)-I(A,B),其中I(A,B)是互信息，代表一个随机变量包含另一个随机变量信息量的度量。\\\\\n",
    "信息熵度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望。考虑随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。\n",
    "另一种说法是：熵是表示随机变量不确定性的度量。  \n",
    "即公式为：$$H(X)=-\\sum_{i=1}^n p(x_i) \\log p(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.决策树模型   \n",
    "决策树是一种基本的分类与回归方法。这里我们主要讨论分类决策树。  \n",
    "决策树学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。  \n",
    "### 2.1 决策树模型  \n",
    "**定义**： 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。    \n",
    "- 决策树与条件概率分布  \n",
    "决策树还表示给定特征条件下类的条件概率分布。这一条件分布定义在特征空间的一个划分上。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成,表示为P(Y|X)。X取值给定划分单元下的集合，Y取值于类的集合。决策树分类将该结点的实例分到条件概率大的那一类去。  \n",
    "\n",
    "### 2.2 决策树学习\n",
    "决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。  本质上是从训练数据集中归纳出一组分类规则。  \n",
    "决策树学习用损失函数表示这一目标。通常，决策树学习的损失函数是正则化的极大似然函数。策略：损失函数最小化。  \n",
    "决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据有一个最好的分类的过程。  \n",
    "递归过程持续进行，直至所有训练数据子集被正确分类或者没有合适的特征为止。  \n",
    "但这一生成的决策树可能对训练数据有很好的分类能力，但对未知数据未必有很好的分类能力，可能发生过拟合现象。因此需要对已生成的树进行自下而上的剪枝，使树变得简单，从而有更好的泛化能力。  \n",
    "**决策树的生成对应于模型的局部选择(局部最优)，决策树的剪枝对应于模型的全局选择(全局最优)。**  \n",
    "常用的算法有ID3,C4.5和 CART。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征选择  \n",
    "特征选择在于选取对训练数据具有分类能力的特征。通常特征选择的准则是**信息增益或信息增益比**。  \n",
    "特征选择是决定用哪个特征来划分特征空间。    \n",
    "### 3.1 信息增益\n",
    "**定义(信息增益)**： 特征A对训练数据集D的信息增益 g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：  \n",
    "$$ g(D,A) = H(D) - H(D|A)$$  \n",
    "一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息。\n",
    "信息增益表示得知特征A的信息而使得数据集D的分类的不确定性减少的程度。  \n",
    "对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。  - 根据信息增益准则的特征选择方法是：对训练数据集D，计算每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。  \n",
    "### 3.2 信息增益比  \n",
    "以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征问题。使用信息增益比可以对这一问题进行校正。  \n",
    "**定义(信息增益比)**：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵$H_A(D)$之比，即:  \n",
    "$$g_R(D,A)=\\dfrac{g(D,A)}{H_A(D)}$$  \n",
    "其中，$H_A(D) = -\\sum_{i=1}^n\\dfrac{D_i}{D}\\log_2\\dfrac{D_i}{D}$, n是特征A取值的个数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.ID3算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
