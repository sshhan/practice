{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本单元基于二分类问题，也就是输出只有两种可能的结果。  \n",
    "利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。  \n",
    "一个事件的几率(odds) 是指该事件发生的概率与不发生的概率的比值。  \n",
    "如果该事件发生概率是P，则该事件的对数几率是$\\dfrac{P}{1-P}$,该事件的对数几率或logit函数是$$logit(P)=log \\dfrac{P}{1-P}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic function的定义\n",
    "logistic函数是有意义的，因为当输入任何实值t,($t\\in R$),输出值会在0-1之间，可以作为概率解释。  \n",
    "标准logstic函数定义如下：\n",
    "$$\\delta(t)=\\dfrac{e^t}{e^t+1}=\\dfrac{1}{1+e^{-t}}$$\n",
    "我们假定t是一个单一解释变量x的线性函数(也可以是多元解释变量的线性组合).  \n",
    "令：$t=\\beta_0+\\beta_1 x$  \n",
    "则logistic函数可以写成：$F(x)=\\dfrac{1}{1+e^{-(\\beta_0+\\beta_1x)}}$  \n",
    "这里F(x)被认为是因变量'成功'的概率。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 详细理解Logistic Regression  \n",
    "直觉上，一个线性模型的输出值y越大，这个事件P(Y=1|x)发生的概率就越大。  \n",
    "而联系事件发生的几率，odds的值域是0到正无穷，几率越大，发生的可能性越大。\n",
    "结合二者，我们再利用logit函数，将概率控制在[0,1]之内，得到如下模型： \n",
    "$log \\dfrac{P}{1-P}= w^T x$  \n",
    "这里$w^Tx=w_0+{w_1}^Tx$    \n",
    "这也就是说，在logistic回归模型中，输出Y=1的对数几率是输入x的线性函数,  \n",
    "或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型。  \n",
    "进而可以求出概率P关于$w^Tx$的概率形式：  \n",
    "$$P(Y=1 | x)=\\dfrac{1}{1+e^{-w^Tx}}=\\dfrac{e^{w^Tx}}{1+e^{w^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数估计\n",
    "利用logistic回归模型学习时，对于给定的训练数据集 $T={\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\\}$,\n",
    "其中，$x_i\\in R^n, y_i\\in {\\{0,1}\\}$,可以应用极大似然估计法估计模型参数。  \n",
    "设:$$P(Y=1|x)=\\pi(x), P(Y=0|x)=\\pi(x)$$\n",
    "似然函数为:$$L(w)=\\prod_{i=1}^n[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$$\n",
    "对数似然函数为: \n",
    "\\begin{align*}\n",
    "l(w)&=\\sum_{i=1}^n[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]\\\\\n",
    "&=\\sum_{i=1}^n [y_ilog\\dfrac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]\\\\\n",
    "&=\\sum_{i=1}^n [y_i (w^Tx_i)-log (1+e^{w^Tx_i})]\n",
    "\\end{align*}\n",
    "这里，$w={\\{w_{0},w_{1}}\\}$,并且我们假定输入向量$x_i$包含一个常数项1，以便接纳截距。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$l(w)$求极大值，得到w的估计值，我们令它的导数等于零，得到如下得分方程:\n",
    "$$\\dfrac{\\partial {l(w)}}{\\partial{w}}=\\sum_{i=1}^n x_i(y_i-\\pi(x_i;w))=0$$\n",
    "这是p+1个w上的非线性方程。注意，由于$x_i$的第一个分量为1，第一个得分方程可确定为：\n",
    "$\\sum_{i=1}^n y_i =\\sum_{i=1}^n\\pi(x_i;w)$;类1的期望数与观测数匹配，类2也如此。  \n",
    "显然，该方程没有显示解。我们可以利用梯度上升或者梯度下降法(也叫最速下降)，来寻找最佳系数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式写成矩阵形式为$$\\dfrac{\\partial {l(w)}}{\\partial{w}}=X^T(y-P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度上升和梯度下降\n",
    "梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。同理，梯度下降是沿着负梯度方向来寻找函数最小值。  \n",
    "由于初始点不同，获得的最大值和最小值也不同，因此二者最终都只能找到局部最优的结果。  \n",
    "梯度上升通常用来寻找似然函数的最大值，梯度下降则用于寻找使损失函数的最小值。  \n",
    "\n",
    "梯度算子：\n",
    "\\begin{equation}\n",
    "\\nabla f(x,y)=\\left(\n",
    "\\begin{array}{c}\n",
    "\\dfrac{\\partial{f(x,y)}}{\\partial{x}}\\\\\n",
    "\\dfrac{\\partial{f(x,y)}}{\\partial{y}}\\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "这个梯度意味着要沿着x的方向移动$\\dfrac{\\partial{f(x,y)}}{\\partial{x}}$,沿着y的方向移动$\\dfrac{\\partial{f(x,y)}}{\\partial{y}}$,其中，f(x,y)必须要在待计算的点上有定义并且可微。  \n",
    "梯度算子总是指向函数值增长最快的方向。梯度上升算法到达每个点后都会重新估计移动的方向。这里所说的是移动方向，移动量的大小我们用步长$\\alpha$表示。迭代过程中，梯度算子总能保证我们能选取到最佳的移动方向。  \n",
    "\n",
    "用向量来表示的话，梯度上升算法的迭代公式为：\n",
    "$$w:= w+\\alpha \\nabla_w f(w)$$\n",
    "在本问题中：$$w:= w+\\alpha\\sum_{i=1}^n x_i(y_i-\\pi(x_i;w))$$\n",
    "梯度下降算法的迭代公式为(只是把加法变成减法)：\n",
    "$$w:= w-\\alpha \\nabla_w f(w)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度上升\n",
    "梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征那么该方法的计算复杂度就太高了。 一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。  \n",
    "由于可以在样本到来时对分类器进行增量式更新，因此随机梯度上升算法是一个在线学习算法。  \n",
    "随机梯度上升可以写成如下的伪代码：  \n",
    "  \n",
    "所有回归系数初始化为1  \n",
    "对数据集中每个样本  \n",
    "       $\\quad$计算该样本的梯度  \n",
    "       $\\quad$使用alpha * gradient更新回归系数值  \n",
    "返回回归系数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson算法(牛顿迭代)\n",
    "#### 原理理解\n",
    "当前任务是优化一个目标函数f,也就是求该函数的极大值或极小值问题，可以转化为求解函数f的导数f'=0点问题。  \n",
    "为了求解方程f'=0的根，把原函数f(x)做泰勒展开，展开到二阶形式：  \n",
    "$$f(x+\\Delta x)=f(x)+f'(x)\\Delta x+\\dfrac{1}{2}f''(x)\\Delta x^2$$\n",
    "当且仅当$\\Delta x$无限趋近于0时，等式成立。此时，上式等价于：  \n",
    "$$f'(x)+\\dfrac{1}{2}f''(x)\\Delta x=0$$\n",
    "注意，因为$\\Delta x$无限趋近于0，前面的常数1/2将不再起作用，可以将其一并忽略，即  \n",
    "$$\\Delta x=-\\dfrac{f'(x)}{f''(x)}$$\n",
    "得出迭代公式：\n",
    "$$x_{n+1}=x_n-\\dfrac{f'(x)}{f''(x)},n=0,1,\\cdots$$\n",
    "#### logistic 分析\n",
    "利用Newton-Raphson算法求解最优参数，这需要二阶导数或者Hessian矩阵。  \n",
    "由$$\\dfrac{\\partial {l(w)}}{\\partial{w}}=\\sum_{i=1}^n x_i(y_i-\\pi(x_i;w))$$可知：  \n",
    "$$\\dfrac{\\partial^2 {l(w)}}{\\partial{w}\\partial{w^T}}=-\\sum_{i=1}^n x_i{x_i}^T\\pi(x_i;w)(1-\\pi(x_i;w))$$\n",
    "以$w^{old}$开始，单个Newton-Raphson更新是：\n",
    "$$ w^{new}=w^{old}-(\\dfrac{\\partial^2 {l(w)}}{\\partial{w}\\partial{w^T}})^{-1}\\dfrac{\\partial {l(w)}}{\\partial{w}}$$\n",
    "其中，导数在$w^{old}$处计算。  \n",
    "将得分和Hessian写成矩阵形式是方便的。  \n",
    "设y表示$y_i$值向量，X是$x_i$值的$n\\times(p+1)$矩阵，P是其第i个元素为$\\pi(x_i;w^{old})$的拟合概率向量，而V是权的$N\\times N$对角矩阵，  \n",
    "第i个对角元素为$\\pi(x_i;w^{old})(1-\\pi(x_i;w^{old}))$  \n",
    "则$\\dfrac{\\partial {l(w)}}{\\partial{w}}=X^T(y-P)，\\dfrac{\\partial^2 {l(w)}}{\\partial{w}\\partial{w^T}}=-X^TVX$  \n",
    "这样，Newton-Raphson的步骤是：\n",
    "\\begin{align*}\n",
    "w^{new}&=w^{old}+(X^TVX)^{-1}X^T(y-P)\\\\\n",
    "&=(X^TVX)^{-1}X^TV(Xw^{old}+V^{-1}(y-P))\\\\\n",
    "&=(X^TVX)^{-1}X^TVz\\\\\n",
    "\\end{align*}\n",
    "在这里，我们使用响应$z=Xw^{old}+V^{-1}(y-P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练算法：使用梯度上升找到最佳参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid 函数的形式：$f(x)=\\dfrac{1}{1+e^{-x}}$, 来源 $\\dfrac{df(x)}{dx}=f(x)(1-f(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradAscent(dataset,classlabel):\n",
    "    datamatrix=mat(dataset)\n",
    "    labelvector=mat(classlabel)\n",
    "    m,n=shape(datamatrix)\n",
    "    alpha=0.001 #步长\n",
    "    maxcircles=500\n",
    "    weights=ones((n,1))\n",
    "    for k in range(maxcircles):\n",
    "        h=sigmoid(datamatrix*weights)\n",
    "        error=labelvector-h\n",
    "        weights=weights+alpha*datamatrix.transpose()*error\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "思考：随机梯度下降和梯度下降迭代一次的计算量问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练算法：随机梯度上升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stocGradAscent(dataMatrix, classlabels):\n",
    "    m,n=shape(dataMatrix)\n",
    "    alpha=0.01\n",
    "    weights=ones(n)\n",
    "    for i in range(m):\n",
    "        h=sigmoid(np.sum(dataMatrix[i]*weights))\n",
    "        error=classlabels[i]-h\n",
    "        weights=weights+alpha*error*datamatrix[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练算法：牛顿迭代法(Newton-Raphson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NewRaphson(dataMatrix,classlabels):\n",
    "    datamat=np.mat(dataMatrix)\n",
    "    m,n=datamat.shape\n",
    "    labelmat=np.mat(classlabels).transpose()\n",
    "    maxcircles=500\n",
    "    weights=np.ones((n,1))\n",
    "    for k in range(maxcircles):\n",
    "        h=sigmoid(datamat*weights)\n",
    "        error=labelmat-h\n",
    "        v=(1-error)*(error.transpose())\n",
    "        weights=weights+(datamat.transpose()*v*datamat).I*datamat.transpose()*h\n",
    "    return weights  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
