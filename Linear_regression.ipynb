{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.广义线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 普通最小二乘\n",
    "线性回归最小化数据中观察到的响应和通过线性逼近预测的响应之间的残差平方和来拟合系数为$\\theta=(\\theta_1,\\cdots,\\theta_p)$的线性模型。从数学上来说，它解决了形式上的问题：  $$J(\\theta)=\\sum_i(y^{(i)}-\\theta^Tx^{(i)})^2$$\n",
    "写成矩阵形式,目标函数如下：\n",
    "$$ \\min\\limits_{\\theta}\\lVert X\\theta - y \\rVert_2^2$$\n",
    "数学推导：\n",
    "\\begin{align*}\n",
    "\\lVert X\\theta - y \\rVert_2^2&=(X\\theta-y)^T(X\\theta-y)\\\\\n",
    "&=(\\theta^TX^T-y^T)(X\\theta-y)\\\\\n",
    "&=\\theta^TX^TX\\theta-2\\theta^TX^Ty+y^Ty\n",
    "\\end{align*}\n",
    "对$\\theta$求导，得$X^TX\\theta-X^Ty$,令其等于0，可以得到系数向量$\\theta$的估计值，如下：  \n",
    "$$\\hat{\\theta}=(X^TX)^{-1}X^Ty$$\n",
    "上述公式包含矩阵$\\ X^TX$的逆，因此这个估计只在该逆矩阵存在的情况下适用。    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ols_fit(X,y):\n",
    "    Xmat=np.mat(X);ymat=np.mat(y).T\n",
    "    xTx=Xmat.T*Xmat\n",
    "    if np.linalg.det(xTx) == 0:\n",
    "        print('This matrix is singular')\n",
    "        return\n",
    "    Theta=xTx.I*(Xmat.T*ymat)\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意：\n",
    "普通最小二乘的系数估计依赖于模型项的独立性。当各项具有相关性且设计矩阵x的列具有近似线性相关时，设计矩阵接近于奇异，因此，最小二乘估计变得对所观察到的响应中的随机误差高度敏感，产生大的方差。这种情况可能出现多重共线性。以下将说明这种情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当解集$\\omega$对X,y的值十分敏感时，那么这样的方程组就是病态的。X称为病态矩阵。仅仅是在X上和y上进行了微小变动，得出的系数值就差异巨大。如下例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This matrix is singular\n",
      "w1 is :\n",
      " [[ 1.  0.]]\n",
      "w2 is :\n",
      " [[ 0.  1.]]\n",
      "w3 is :\n",
      " [[ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "X=[[0,0],[1,1],[2,2]]\n",
    "y=[0,1,2]\n",
    "w1=ols_fit(X,y)\n",
    "X1=[[0,0],[1,1.1],[2,2]]\n",
    "y1=[0,1,2]\n",
    "w1=ols_fit(X1,y1)\n",
    "print('w1 is :\\n',w1.T)\n",
    "X2=[[0,0],[1,1],[2.1,2]]\n",
    "y2=[0,1,2]\n",
    "w2=ols_fit(X2,y2)\n",
    "print('w2 is :\\n',w2.T)\n",
    "X3=[[0,0],[1,1.1],[2,2]]\n",
    "y3=[0,1.1,2]\n",
    "w3=ols_fit(X3,y3)\n",
    "print('w3 is :\\n',w3.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降算法解决这种问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 局部加权线性回归(LWLR)\n",
    "现实生活中很多数据采用线性模型(出现欠拟合)不能很好的描述，比如说房价的预测问题，直线不能很好的拟合所有的数据点，甚至存在较大的误差，我们可能采用一条类似于二次函数的曲线可以拟合的更好。但是为了解决在非线性模型里建立线性模组的问题，我们预测一点的值时，选择与这个点相近的点而不是所有的点做线性回归。基于此，产生了局部加权回归算法。  \n",
    "在该算法中，我们给待预测点附近的每一个点赋予一定的权重，在这个子集上基于最小均方差进行普通的回归。与knn一样，这种算法每次预测时均需要事先选取出对应的数据子集。\n",
    "残差平方和(损失函数)变为:\n",
    "$$J(\\theta)=\\sum_i\\omega^{(i)}((y^{(i)}-\\theta^Tx^{(i)})^2$$\n",
    "写成矩阵形式，目标函数如下：\n",
    "$$\\min\\limits_w(X\\theta-y)^TW(X\\theta-y)$$\n",
    "数学推导如下：\n",
    "\\begin{align*}\n",
    "(X\\theta-y)^TW(X\\theta-y)&=(\\theta^TX^T-y^T)W(X\\theta-y)\\\\\n",
    "&=\\theta^TX^TWX\\theta-2\\theta^TX^TWy+y^TWy\\\\\n",
    "\\end{align*}\n",
    "对函数求导，得：$X^TWX\\theta-X^TWy$,令其等于0，得到回归系数的估计值如下：\n",
    "$$\\hat{\\theta}=(X^TWX)^{-1}X^TWY$$\n",
    "LWLR使用‘核’来对附近的点赋予更高的权重。核的类型可以自由选择，最常见的核就是高斯核，对应的权重如下：\n",
    "$$\\omega^{(i)}=exp(-\\dfrac{(x^(i)-x)^2}{-2k^2})$$\n",
    "这样就构建了一个只含对角元素的权重矩阵，并且x与$x^{(i)}$点越近权重越大。其中，k为波长参数，它控制了权值随距离下降的速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "在使用这个算法训练数据时，不仅需要学习线性回归的参数，还要学习波长参数。这个算法的问题在于，对于每一个要预测的点，都要依据整个数据集计算一个线性回归模型出来，使得算法代价极高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def lwlr(testpoint,X,y,k=1.0):\n",
    "    Xmat=np.mat(X);ymat=np.mat(y).T\n",
    "    m=Xmat.shape[0]\n",
    "    weights=np.mat(np.eye((m)))\n",
    "    for i in range(m):\n",
    "        diffmat=testpoint-Xmat[i,:]\n",
    "        weights[i,i]=np.exp(diffmat*diffmat.T/(-2.0*k**2))\n",
    "    xTwx=Xmat.T*weights*Xmat\n",
    "    if np.linalg.det(xTwx)==0:\n",
    "        print('This matrix is singula')\n",
    "        return\n",
    "    theta=xTwx.I*(Xmat.T*weights*ymat)\n",
    "    return testpoint*theta\n",
    "def lwlrtest(testArr,X,y,k=1.0):\n",
    "    m=testArr.shape[0]\n",
    "    y_pred=np.zeros(m)\n",
    "    for i in range(m):\n",
    "        y_pred[i]=lwlr(testArr[i],X,y,k)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 衡量效果 方差分数\n",
    "## 优缺点 \n",
    "优点：结果易于理解，计算不复杂  \n",
    "缺点：对非线性数据拟合不好  \n",
    "适用数据类型：数值型和标称型  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 之后要介绍学习曲线的东西\n",
    "测试集跟训练集的学习曲线的接近程度   \n",
    "解释欠拟合，过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
