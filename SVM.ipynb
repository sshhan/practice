{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM 支持向量机\n",
    "支持向量机(SVMs)是用于分类，回归，和异常值检测的一系列监督学习方法的集合。  \n",
    "优点：  \n",
    "1.在高维空间上有效；  \n",
    "2.在p>n(也就是特征数目-维数大于样本数)的情形下仍然有效；  \n",
    "3.在决策函数上使用训练数据点的一个子集(称为支撑向量), 空间占用少；  \n",
    "4.Versatile(万能性):决策函数可以使用不同的核函数(kernel function)。  \n",
    "缺点：  \n",
    "1.如果特征数大于样本数，为避免过度拟合需要选择合适的核函数和正则项；  \n",
    "2.SVMs并不直接提供概率估计，计算使用五折交叉验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.间隔与支撑向量\n",
    "给定训练样本集$D={\\{(x_1,y_1),\\cdots,(x_m,y_m}\\},y_i\\in{\\{-1,1}\\}$，分类学习的基本想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。  \n",
    "能将训练样本分开的划分超平面有很多，如何寻找最优的？   \n",
    "\n",
    "**思想： 间隔最大化**   \n",
    "这里使用几何间隔而不是函数间隔。  \n",
    "\n",
    "在样本空间中，划分超平面可通过如下线性方程来描述：  \n",
    "$$w^Tx+b=0$$  \n",
    "其中,$w=(w_1,\\cdots,w_d)'$为法向量，决定了超平面的方向，b为位移项，决定了超平面与原点之间的距离。显然，划分超平面可以被法向量w和位移b决定。  \n",
    "在样本空间中，任意点x到超平面的距离可以写为：  \n",
    "$$r=\\dfrac{|w^Tx+b|}{\\lVert w\\rVert}$$  \n",
    "假设超平面将训练样本正确分类，即对于$(x_i,y_i)\\in D$,若$y_i=+1$,则有$w^Tx_i+b>0$；若$y_i=-1$,则有$w^Tx_i+b<0$.令：  \n",
    "$$ \\begin{cases}\n",
    "w^Tx_i+b\\ge 1, & y_i=+1 \\\\\n",
    "w^Tx_i+b\\le -1, & y_i=-1 \n",
    "\\end{cases}\\qquad(*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为\"支持向量\"，两个异类支持向量到超平面的距离之和为： \n",
    "$$r = \\dfrac{2}{\\lVert w\\rVert}$$  \n",
    "它被称为间隔。  \n",
    "### 1.1从原始问题到对偶问题\n",
    "要找到具有\"最大间隔\"的划分超平面，也就是要找到满足(*)式约束中的参数w和b,使得r最大，即：  \n",
    "\n",
    "$$\\max_{w,b}\\dfrac{2}{\\lVert w\\rVert}$$\n",
    "$$ s.t. y_i(w^Tx_i+b)\\ge 1,\\quad i=1,\\cdots,m$$  \n",
    "显然，为了最大化间隔，仅需最大化$\\lVert w\\rVert^{-1}$,这就等价于最小化$\\lVert w\\rVert^2$.于是，将目标函数重写为：  \n",
    "\n",
    "$$\\min_{w,b}\\dfrac{1}{2}{\\lVert w\\rVert}^2\\qquad(**)$$\n",
    "$$ s.t. y_i(w^Tx_i+b)\\ge 1,\\quad i=1,\\cdots,m$$   \n",
    "这就是支持向量机(Support Vector Machine)的基本形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.对偶问题的求解\n",
    "我们希望求解$(**)$式来得到最大间隔划分超平面所对应的模型。$(**)$本身是一个凸的二次规划问题，本身可以用现成的优化计算包求解，但我们有更高效的办法。  \n",
    "\n",
    "由于这个问题的特殊结构，还可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法。  \n",
    "使用拉格朗日乘子法的优点在于：  \n",
    "1. 对偶问题往往更容易求解；  \n",
    "2. 可以自然的引入核函数，进而推广到非线性分类问题。  \n",
    "### 拉格朗日乘子法\n",
    "通过对每一个约束条件加上一个拉格朗日乘子$\\alpha_i\\ge 0$，定义拉格朗日函数：  \n",
    "$$L(w,b,\\alpha)=\\dfrac{1}{2}{\\lVert w\\rVert}^2 +\\sum_{i=1}^m\\alpha_i(1-y_i(w^Tx_i+b))$$  \n",
    "其中，$\\alpha=(\\alpha_i;\\cdots;\\alpha_m)$。令$L(w,b,\\alpha)$对w和b的偏导为零可得：  \n",
    "$$w=\\sum_{i=1}^m\\alpha_iy_ix_i$$  \n",
    "$$0=\\sum_{i=1}^m\\alpha_iy_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
