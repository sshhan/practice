{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF_IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)。  \n",
    "> 是一种用于信息检索和文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。  \n",
    "\n",
    "上述引用总结就是，一个词语在一篇文章中出现的次数越多，同时在所有文档中出现的次数越少，越能够代表该文章。  \n",
    "\n",
    "** 应用** ： 用来做关键词的抽取，词的TF_IDF值越大，则为关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基本原理\n",
    "** 词频(term frequency, TF)** 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数)，以防止它偏向长的文件。(同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否)    \n",
    "- 表示关键词w在文档$D_i$中出现的频率：  \n",
    "$$TF_{w,D_i}=\\dfrac{\\mathrm{count}(w)}{|D_i|}$$  \n",
    "其中，$\\mathrm{count}(w)$为关键词w的出现次数，$|D_i|$为$|D_i|$中所有词的数量。  \n",
    "\n",
    "\n",
    "** 逆向文件频率(inverse document frequency, IDF)** 主要思想是：如果包含词条t的文档越少，IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数据除以包含该词语的文件的数据，再将得到的商取对数得到。  \n",
    "- 反映关键词的普遍程度-当一个词越普遍(即有大量文档包含这个词)时，其IDF值越低；反之，IDF值越高。定义如下：  \n",
    "$$IDF_w=\\log\\dfrac{N}{\\sum_{i=1}^N I(w,D_i)}$$  \n",
    "其中，N为所有的文档总数，$I(w,D_i)$表示文档$D_i$是否包含关键词w，若包含则为1，否则为0.若词w在所有文档中均未出现，则IDF公式中的分母为0；因此需要对IDF进行平滑：  \n",
    "$$IDF_w=\\log\\dfrac{N}{1+\\sum_{i=1}^N I(w,D_i)}$$  \n",
    "\n",
    "\n",
    "关键词w在文档$D_i$的**TF-IDF**值为：   \n",
    "\n",
    "$${TF-IDF}_{w,D_i}=TF_{w,D_i}*IDF_w$$  \n",
    "### 小结：  \n",
    "1. 当一个词在文档频率越高并且新鲜度高(即普遍度低)，其TF-IDF值越高；  \n",
    "2. TF-IDF兼顾词频与新鲜度，过滤一些常见词，保留能提供更多信息的重要词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Python应用实例\n",
    "### Basic term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('loves', 2), ('more', 1), ('Linda', 1), ('than', 1), ('me', 2), ('Julie', 1)])\n",
      "dict_items([('loves', 1), ('more', 1), ('Jane', 1), ('than', 1), ('likes', 1), ('me', 2), ('Julie', 1)])\n",
      "dict_items([('baseball', 1), ('more', 1), ('than', 1), ('basketball', 1), ('He', 1), ('likes', 1)])\n"
     ]
    }
   ],
   "source": [
    "# examples taken from here: http://stackoverflow.com/a/1750187\n",
    "# 计算词频\n",
    "mydoclist = ['Julie loves me more than Linda loves me',\n",
    "           'Jane likes me more than Julie loves me',\n",
    "           'He likes basketball more than baseball']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] += 1\n",
    "    print(tf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [baseball,more,Jane,He,Linda,loves,than,basketball,likes,me,Julie]\n",
      "The doc is \"Julie loves me more than Linda loves me\"\n",
      "the tf vector for Document 1 is [0,1,0,0,1,2,1,0,0,2,1]\n",
      "The doc is \"Jane likes me more than Julie loves me\"\n",
      "the tf vector for Document 2 is [0,1,1,0,0,1,1,0,1,2,1]\n",
      "The doc is \"He likes basketball more than baseball\"\n",
      "the tf vector for Document 3 is [1,1,0,1,0,0,1,1,1,0,0]\n",
      "All combined, here is our master document term matrix:\n",
      "[[0, 1, 0, 0, 1, 2, 1, 0, 0, 2, 1], [0, 1, 1, 0, 0, 1, 1, 0, 1, 2, 1], [1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# 词的集合\n",
    "def build_lexicon(corpus):\n",
    "    lexicon=set()\n",
    "    for doc in corpus:\n",
    "        for word in doc.split():\n",
    "            lexicon.update(word)\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "# 词在一个文档中的频率\n",
    "def tf(term,document):\n",
    "    return freq(term,document)\n",
    "\n",
    "def freq(term,document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "# 词典 key-vocabulary \n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "print('Our vocabulary vector is ['+','.join(list(vocabulary))+']')\n",
    "\n",
    "for doc in mydoclist:\n",
    "    print('The doc is \"' +doc+ '\"')\n",
    "    tf_vector =[tf(word,doc) for word in vocabulary]\n",
    "    tf_vector_string = ','.join(format(freq,'d') for freq in tf_vector)\n",
    "    print ('the tf vector for Document %d is [%s]'%((mydoclist.index(doc)+1),tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "\n",
    "print('All combined, here is our master document term matrix:')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing vectors to L2 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix:\n",
      "[[0 1 0 0 1 2 1 0 0 2 1]\n",
      " [0 1 1 0 0 1 1 0 1 2 1]\n",
      " [1 1 0 1 0 0 1 1 1 0 0]]\n",
      "\n",
      "A document term matrix with row-wide L2 norms of 1\n",
      "[[ 0.          0.28867513  0.          0.          0.28867513  0.57735027\n",
      "   0.28867513  0.          0.          0.57735027  0.28867513]\n",
      " [ 0.          0.31622777  0.31622777  0.          0.          0.31622777\n",
      "   0.31622777  0.          0.31622777  0.63245553  0.31622777]\n",
      " [ 0.40824829  0.40824829  0.          0.40824829  0.          0.\n",
      "   0.40824829  0.40824829  0.40824829  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom= np.sum([el**2 for el in vec])\n",
    "    return [(el/math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('A regular old document term matrix:')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "\n",
    "print('\\nA document term matrix with row-wide L2 norms of 1')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF frequency weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [baseball,more,Jane,He,Linda,loves,than,basketball,likes,me,Julie]\n",
      "The inverse document frequency vector is [0.405465,-0.287682,0.405465,0.405465,0.405465,0.000000,-0.287682,0.405465,0.000000,0.000000,0.000000]\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount=0\n",
    "    for doc in doclist:\n",
    "        if freq(word,doc)>0:\n",
    "            doccount += 1\n",
    "    return doccount\n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples=len(doclist)\n",
    "    df = numDocsContaining(word,doclist)\n",
    "    return np.log(n_samples/(1+df))\n",
    "my_idf_vector = [idf(word,mydoclist) for word in vocabulary]\n",
    "\n",
    "print('Our vocabulary vector is ['+','.join(list(vocabulary))+']')\n",
    "print('The inverse document frequency vector is ['+','.join(format(freq,'f') for freq in\n",
    "                                                           my_idf_vector)+']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将idf_vector 变成 idf_matrix，对角线元素为向量元素\n",
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat=np.zeros((len(idf_vector),len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat,idf_vector)\n",
    "    return idf_mat\n",
    "my_idf_matrix=build_idf_matrix(my_idf_vector)\n",
    "#print(my_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF matrix is: [[ 0.         -0.08304666  0.          0.          0.11704769  0.\n",
      "  -0.08304666  0.          0.          0.          0.        ]\n",
      " [ 0.         -0.09097306  0.12821933  0.          0.          0.\n",
      "  -0.09097306  0.          0.          0.          0.        ]\n",
      " [ 0.16553044 -0.11744571  0.          0.16553044  0.          0.\n",
      "  -0.11744571  0.16553044  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf = np.dot(doc_term_matrix_l2,my_idf_matrix)\n",
    "print('TF_IDF matrix is:',doc_term_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用sklearn实现TF-IDF  \n",
    "**注意**：   \n",
    "与上面过程不同之处在于IDF的计算，sklearn中以公式：  \n",
    "\n",
    "$$IDF_w=\\log\\dfrac{N}{\\sum_{i=1}^N I(w,D_i)}+1$$  \n",
    "进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'baseball': 0, 'more': 9, 'julie': 4, 'jane': 3, 'linda': 6, 'loves': 7, 'than': 10, 'basketball': 1, 'he': 2, 'likes': 5, 'me': 8}\n",
      "[[ 0.          0.          0.          0.          0.28945906  0.\n",
      "   0.38060387  0.57891811  0.57891811  0.22479078  0.22479078]\n",
      " [ 0.          0.          0.          0.41715759  0.3172591   0.3172591\n",
      "   0.          0.3172591   0.6345182   0.24637999  0.24637999]\n",
      " [ 0.48359121  0.48359121  0.48359121  0.          0.          0.36778358\n",
      "   0.          0.          0.          0.28561676  0.28561676]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df = 1)\n",
    "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "print('Vocabulary:', count_vectorizer.vocabulary_)  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "tfidf.fit(term_freq_matrix)\n",
    "\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "print(tf_idf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.28945906  0.\n",
      "   0.38060387  0.57891811  0.57891811  0.22479078  0.22479078]\n",
      " [ 0.          0.          0.          0.41715759  0.3172591   0.3172591\n",
      "   0.          0.3172591   0.6345182   0.24637999  0.24637999]\n",
      " [ 0.48359121  0.48359121  0.48359121  0.          0.          0.36778358\n",
      "   0.          0.          0.          0.28561676  0.28561676]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "print(tfidf_matrix.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
