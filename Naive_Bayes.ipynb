{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯\n",
    "朴素贝叶斯方法是基于贝叶斯定理与特征条件独立假设的分类方法。首先机遇特征条件独立假设学习输入／输出的联合概率分布；任何机遇此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。这种方法的思想真的很“朴素”。\n",
    "## 1.基本方法\n",
    "给定训练数据集$$T={\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}\\}$$  \n",
    "其中每个样本$x_i$都包括n维特征，即$x_i=(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})$,类别集合含有K种类别，即$y_i\\in\\mathscr{y}=(c_1,c_2,\\cdots,c_K)$属于K类中的一类。  \n",
    "现在来了一个新样本x，我们要如何判断它的类别？从概率的角度来讲，这个问题等价于给定x，它属于哪个类别的概率最大。那么问题转化为求解$P(y=c_1|x),P(y=c_2|x),\\cdots,P(y=c_K|x)$中最大的那一个，即求后验概率最大的输出：$$argmax P(y=c_k|x)\\qquad (*)$$\n",
    "### 1.1贝叶斯公式\n",
    "$$P(Y|X)=\\dfrac{P(X|Y)\\times P(Y)}{P(X)}$$  \n",
    "### 1.2特征条件独立假设('朴素')\n",
    "给定目标值时各特征之间相互条件独立，等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。\n",
    "### 1.3学习过程\n",
    "朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y).具体地，学习以下先验分布和条件概率分布。先验分布：\n",
    "$$P(Y=c_k),\\qquad k=1,2,\\cdots,K$$\n",
    "条件概率分布\n",
    "$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k),\\quad,k=1,\\cdots,K$$\n",
    "于是学习到联合概率分布P(X,Y)。  \n",
    "## 注意：\n",
    "条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，假设$x^{(j)}$可取值有$S_j$个，$j=1,2,\\cdots,n$,Y可取值有K个，那么参数的个数为$K\\prod_{j=1}^nS_j$.  \n",
    "这显然不可行。针对这个问题，朴素贝叶斯方法对条件概率分布作了条件独立性假设.  \n",
    "在特征独立性假设的前提下，条件概率可以转化为：\n",
    "$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)=\\prod_{j=1}^nP(x^{(j)}|Y=c_k)\\qquad k=1,\\cdots,K$$\n",
    "这样，参数的个数就降到$\\sum_{j=1}^nS_jK$。    \n",
    "结合贝叶斯公式，计算(*)式所给的后验概率，并将后验概率最大的类作为x的类输出。   \n",
    "  \n",
    "\\begin{align}\n",
    "P(Y=c_k|X=x)&=\\dfrac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}\\\\\n",
    "&=\\dfrac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_kP(X=x|Y=c_k)P(Y=c_k)}\\\\\n",
    "&=\\dfrac{\\prod_{j=1}^nP(x^{(j)}|Y=c_k)P(Y=c_k)}{\\sum_k\\prod_{j=1}^nP(x^{(j)}|Y=c_k)P(Y=c_k)},\\qquad k=1,\\cdots,K\n",
    "\\end{align}\n",
    "这是朴素分类器的基本公式，注意到，上式中分母对于所有$c_k$都是相同的，所以目标函数简化为：  \n",
    "$$y=arg\\max\\limits_{c_k}P(Y=c_k)\\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 后验概率最大化的含义\n",
    "\n",
    "朴素贝叶斯法将实例分到后验概率最大的类中，这等价于0-1损失函数时期望风险最小化。\n",
    "$$L(Y,f(X))=\\begin{cases}\n",
    "1,&Y\\neq f(X)\\\\\n",
    "0,&Y=f(X)\n",
    "\\end{cases}$$  \n",
    "其中f(X)是分类决策函数。这时，关于联合分布P(X,Y)取的期望风险函数为：  \n",
    "  \n",
    "$$R(f)=E[L(Y,f(X))]$$  \n",
    "取条件期望有(这里X给定，则类别$c_k$已知)：\n",
    "$$R(f)=E_XE[L(c_k,f(X))|X]=E_X\\sum_{k=1}^KL(c_k,f(X))P(c_k|X)$$\n",
    "(由条件期望性质得来：$E[g(X,Y)]=E_XE[g(X,Y)|X]$)\n",
    "为了使期望风险最小化，只需对X=x逐个极小化，由此可得：  \n",
    "\\begin{align}\n",
    "f(x)&=arg\\min\\limits_{y\\in\\mathscr{y}}\\sum_{k=1}^KL(c_k,y)P(c_k|X=x)\\\\\n",
    "&=arg\\min\\limits_{y\\in\\mathscr{y}}\\sum_{k=1}^KP(y\\neq c_k|X=x)\\\\\n",
    "&=arg\\min\\limits_{y\\in\\mathscr{y}}(1-P(y= c_k|X=x))\\\\\n",
    "&=arg\\max\\limits_{y\\in\\mathscr{y}}(P(y= c_k|X=x))\n",
    "\\end{align}\n",
    "这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：\n",
    "$$f(x)=arg\\max\\limits_{y\\in\\mathscr{y}}(P(y= c_k|X=x))$$\n",
    "即贝叶斯法所采用的原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.朴素贝叶斯的三个常用模型：高斯、多项式、伯努利"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
