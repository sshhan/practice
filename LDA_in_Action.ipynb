{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LDA 降维算法\n",
    "- LDA降维一般分为5个步骤：\n",
    "> 1. 计算数据集中每个类别的均值向量；  \n",
    "> 2. 通过均值向量，计算类间散度矩阵$S_B$和类内散度矩阵$S_W$;  \n",
    "> 3. 对$S_W^{-1}S_BW=\\lambda W$进行特征值求解，求出$S_W^{-1}S_B$的特征向量和特征值；  \n",
    "> 4. 对特征向量按照特征值的大小降序排列，并选择前K个特征向量组成投影矩阵W；  \n",
    "> 5. 通过$D\\times K$ 维的特征值将样本点投影到新的子空间中，$Y = X\\times W$\n",
    "\n",
    "### 1. 二分类下的LDA\n",
    "**编程实现线性判别，并给出西瓜数据集3.0$\\alpha$上的结果。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "# 输入西瓜数据集\n",
    "watermelon= np.array([[0.697,0.460,'是'],[0.774,0.376,'是'],[0.634,0.264,'是'],\n",
    "           [0.608,0.318,'是'],[0.556,0.215,'是'],[0.403,0.237,'是'],\n",
    "           [0.481,0.149,'是'],[0.437,0.211,'是'],[0.666,0.091,'否'],\n",
    "           [0.243,0.267,'否'],[0.245,0.057,'否'],[0.343,0.099,'否'],\n",
    "           [0.639,0.161,'否'],[0.657,0.198,'否'],[0.360,0.370,'否'],\n",
    "           [0.593,0.042,'否'],[0.719,0.103,'否']])\n",
    "\n",
    "# 数据处理-提取特征和类别\n",
    "features = watermelon[:,0:2].astype('float')\n",
    "labels = watermelon[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用sklearn 实现LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n",
      "[[3 2]\n",
      " [1 3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          否       0.75      0.60      0.67         5\n",
      "          是       0.60      0.75      0.67         4\n",
      "\n",
      "avg / total       0.68      0.67      0.67         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size=0.5,random_state=0 )\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "score = accuracy_score(y_test, pred_test)\n",
    "\n",
    "\n",
    "print(score)\n",
    "print(confusion_matrix(y_test, pred_test))\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动编程实现LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.6666666666666666\n",
      "[[3 2]\n",
      " [1 3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          否       0.75      0.60      0.67         5\n",
      "          是       0.60      0.75      0.67         4\n",
      "\n",
      "avg / total       0.68      0.67      0.67         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算类均值\n",
    "def class_mean(features_raw, labels_raw):\n",
    "    mean_vectors = []\n",
    "    for i in np.unique(labels_raw):\n",
    "        mean_vectors.append(np.mean(features_raw[labels_raw==i],axis = 0))\n",
    "    return mean_vectors\n",
    "\n",
    "# 计算类内散度\n",
    "def within_scatter(features_raw, labels_raw):\n",
    "    m = features_raw.shape[1]\n",
    "    mean_vector = class_mean(features_raw, labels_raw)\n",
    "    S_w = np.zeros((m,m))\n",
    "    for i, mean in zip(np.unique(labels),mean_vector):\n",
    "        mean = np.mat(mean).T\n",
    "        feature = features_raw[labels_raw==i]\n",
    "        S_w +=np.dot(feature.T - np.tile(mean,len(feature)),feature - np.tile(mean,len(feature)).T)\n",
    "    return S_w\n",
    "\n",
    "\n",
    "def lda_2_class_project_mean(features_raw, labels_raw):\n",
    "    Sw = within_scatter(features_raw, labels_raw)\n",
    "    U, sigma, VT = np.linalg.svd(np.mat(Sw))\n",
    "    Sw_inv = VT.T*np.linalg.inv(np.diag(sigma))*U.T\n",
    "    mean_vector = class_mean(features_raw, labels_raw)\n",
    "    w = np.dot(Sw_inv,np.mat(mean_vector[0]-mean_vector[1]).T)\n",
    "    mean_new = np.zeros((2,1))\n",
    "    mean_new[0] = np.dot(w.T,np.mat(mean_vector[0]).T)\n",
    "    mean_new[1] = np.dot(w.T,np.mat(mean_vector[1]).T)\n",
    "\n",
    "    return w, mean_new\n",
    "\n",
    "def lda_test(features_raw, labels_raw,features_test, labels_test):\n",
    "    w, mean_new = lda_2_class_project_mean(features_raw, labels_raw)\n",
    "    correct = 0\n",
    "    labels_pred = []\n",
    "    for i in range(len(features_test)):\n",
    "        proj = np.dot(w.T,features_test[i])\n",
    "        if abs(proj - mean_new[0]) <= abs(proj - mean_new[1]):\n",
    "            pred = np.unique(labels_raw)[0]\n",
    "            labels_pred.append(np.unique(labels_raw)[0])  \n",
    "        else:\n",
    "            pred = np.unique(labels_raw)[1]\n",
    "            labels_pred.append(np.unique(labels_raw)[1])\n",
    "        if pred == labels_test[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct /len(features_test)\n",
    "    return accuracy,labels_pred\n",
    "\n",
    "# 利用上面sklearn拆分的数据集进行计算\n",
    "acc, y_pred = lda_test(X_train, y_train,X_test, y_test)\n",
    "\n",
    "print ('accuracy is:',acc )\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.多分类下的LDA  \n",
    "这里我们使用Iris数据集。程序来源http://sebastianraschka.com/Articles/2014_python_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dict = {i:label for i,label in zip(range(4),\n",
    "                                                ('sepal length in cm',\n",
    "                                                'sepal width in cm',\n",
    "                                                'petal length in cm',\n",
    "                                                'petal width in cm',))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "145                 6.7                3.0                 5.2   \n",
       "146                 6.3                2.5                 5.0   \n",
       "147                 6.5                3.0                 5.2   \n",
       "148                 6.2                3.4                 5.4   \n",
       "149                 5.9                3.0                 5.1   \n",
       "\n",
       "     petal width in cm     class label  \n",
       "145                2.3  Iris-virginica  \n",
       "146                1.9  Iris-virginica  \n",
       "147                2.0  Iris-virginica  \n",
       "148                2.3  Iris-virginica  \n",
       "149                1.8  Iris-virginica  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读入数据\n",
    "df = pd.io.parsers.read_csv(\n",
    "    filepath_or_buffer = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "    header = None,\n",
    "    sep = ',',)\n",
    "\n",
    "df.columns = [l for i,l in sorted(feature_dict.items())]+['class label']\n",
    "df.dropna(how = 'all', inplace = True) # to drop the empty line at file-end\n",
    "\n",
    "\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把类别转换为1，2，3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = df.values[:,0:4].astype('float')\n",
    "y = df['class label'].values  \n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y) + 1  \n",
    "label_dict = {1:'Setosa', 2:'Versicolor', 3:'Virginica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 步骤：\n",
    "- Step 1: 计算 d-维均值向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Vector class 1: [ 5.006  3.418  1.464  0.244]\n",
      "\n",
      "Mean Vector class 2: [ 5.936  2.77   4.26   1.326]\n",
      "\n",
      "Mean Vector class 3: [ 6.588  2.974  5.552  2.026]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision = 4, suppress = True)\n",
    "\n",
    "mean_vectors = []\n",
    "for i in range(1,4):\n",
    "    mean_vectors.append(np.mean(X[y==i],axis=0))\n",
    "    print('Mean Vector class %s: %s\\n'%(i,mean_vectors[i-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2: 计算散布矩阵\n",
    "> - 类内散布矩阵Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within-class scatter matrix :\n",
      " [[ 38.9562  13.683   24.614    5.6556]\n",
      " [ 13.683   17.035    8.12     4.9132]\n",
      " [ 24.614    8.12    27.22     6.2536]\n",
      " [  5.6556   4.9132   6.2536   6.1756]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision = 4, suppress = True) \n",
    "\n",
    "m = X.shape[1]\n",
    "S_W = np.zeros((m,m))\n",
    "\n",
    "for i, mean in zip(range(1,4), mean_vectors):\n",
    "    feature = X[y==i]\n",
    "    mean = np.mat(mean).T\n",
    "    S_W  +=  np.dot(feature.T - np.tile(mean, len(feature)),feature - np.tile(mean, len(feature)).T)\n",
    "print('within-class scatter matrix :\\n',S_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -    另一种算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within-class scatter matrix :\n",
      " [[ 38.9562  13.683   24.614    5.6556]\n",
      " [ 13.683   17.035    8.12     4.9132]\n",
      " [ 24.614    8.12    27.22     6.2536]\n",
      " [  5.6556   4.9132   6.2536   6.1756]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision = 4)\n",
    "m = X.shape[1]\n",
    "S_w1 = np.zeros((m,m))\n",
    "\n",
    "for i, mean in zip(range(1,4),mean_vectors):\n",
    "    class_sc_mat = np.zeros((m,m))\n",
    "    for row in X[y==i]:\n",
    "        row, mean = row.reshape(m,1),mean.reshape(m,1)\n",
    "        class_sc_mat = class_sc_mat + (row - mean).dot((row-mean).T)\n",
    "    S_w1 = S_w1 + class_sc_mat\n",
    "    \n",
    "print('within-class scatter matrix :\\n',S_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 类间散布矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between-class Scatter Matrix:\n",
      " [[  63.2121  -19.534   165.1647   71.3631]\n",
      " [ -19.534    10.9776  -56.0552  -22.4924]\n",
      " [ 165.1647  -56.0552  436.6437  186.9081]\n",
      " [  71.3631  -22.4924  186.9081   80.6041]]\n"
     ]
    }
   ],
   "source": [
    "S_B = np.zeros((m,m))\n",
    "mean_all = np.mean(X,axis= 0)\n",
    "\n",
    "for i, mean_i in zip(range(1,4),mean_vectors):\n",
    "    m_i = len(X[y==i])\n",
    "    mean_i = mean_i.reshape(m,1)\n",
    "    mean_all = mean_all.reshape(m,1)\n",
    "    S_B =  S_B + m_i *(mean_i - mean_all).dot((mean_i - mean_all).T)\n",
    "    \n",
    "print('between-class Scatter Matrix:\\n', S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3: 求解矩阵$S_W^{-1}S_B$的广义奇异值问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigenvector 1:\n",
      "[[-0.2049]\n",
      " [-0.3871]\n",
      " [ 0.5465]\n",
      " [ 0.7138]]\n",
      "Eigenvalue 1: 3.23e+01\n",
      "\n",
      "Eigenvector 2:\n",
      "[[-0.009 ]\n",
      " [-0.589 ]\n",
      " [ 0.2543]\n",
      " [-0.767 ]]\n",
      "Eigenvalue 2: 2.78e-01\n",
      "\n",
      "Eigenvector 3:\n",
      "[[-0.1997]\n",
      " [-0.2343]\n",
      " [-0.302 ]\n",
      " [ 0.8116]]\n",
      "Eigenvalue 3: 4.62e-15\n",
      "\n",
      "Eigenvector 4:\n",
      "[[-0.1997]\n",
      " [-0.2343]\n",
      " [-0.302 ]\n",
      " [ 0.8116]]\n",
      "Eigenvalue 4: 4.62e-15\n"
     ]
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec = eig_vecs[:,i].reshape(4,1)\n",
    "    print('\\nEigenvector {}:\\n{}'.format(i+1, eigvec.real))\n",
    "    print('Eigenvalue {:}: {:.2e}'.format(i+1, eig_vals[i].real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查-特征值和特征向量的计算-通过$Av=\\lambda v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(eig_vals)):\n",
    "    eigv = eig_vecs[:,i].reshape(4,1)\n",
    "    np.testing.assert_array_almost_equal(np.linalg.inv(S_W).dot(S_B).dot(eigv),\n",
    "                                        eig_vals[i]*eigv,\n",
    "                                         decimal = 6, err_msg =' ',verbose = True)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4: 选择线性判别子构造新的特征空间 \n",
    "> - 将特征向量按照特征值的大小降序排列。线性代数告诉我们，矩阵乘法可以看作一种线性变换，而特征向量和特征值代表了变换后的方向以及该方向上的缩放比例，因此特征值越大，说明这个方向在变换中越显著，也就是信息量最大。因此我们需要抛弃的是特征值较小的方向，因此，我们只需要选取前Top-K个特征值对应的特征向量，就得到了映射矩阵W。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues in decreasing order: \n",
      "\n",
      "32.2719577997 [-0.2049+0.j -0.3871+0.j  0.5465+0.j  0.7138+0.j]\n",
      "0.27756686384 [-0.0090+0.j -0.5890+0.j  0.2543+0.j -0.7670+0.j]\n",
      "5.0228932243e-15 [-0.1997+0.335j  -0.2343-0.1479j -0.3020-0.1457j  0.8116+0.j    ]\n",
      "5.0228932243e-15 [-0.1997-0.335j  -0.2343+0.1479j -0.3020+0.1457j  0.8116-0.j    ]\n"
     ]
    }
   ],
   "source": [
    "# make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pair = [(np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tupls from high to low\n",
    "eig_pair = sorted(eig_pair , key = lambda k: k[0], reverse = True)  \n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "\n",
    "print('Eigenvalues in decreasing order: \\n')\n",
    "for i in eig_pair:\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的特征值可以看到有2个特征值非常接近0，一是代表了它们不包含信息量，而是因为浮点运算的精确度问题。  \n",
    "在LDA中，如果有C类，线性判别式最多只有C-1个，因此对于之前3类的数据集，最多只有2个特征值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: \n",
      "\n",
      "eigvalue1:99.15%\n",
      "eigvalue2:0.85%\n",
      "eigvalue3:0.00%\n",
      "eigvalue4:0.00%\n"
     ]
    }
   ],
   "source": [
    "# 我们通过特征值的比例来体现方差的分布：  \n",
    "print('Variance explained: \\n')\n",
    "eigv_sum = sum(eig_vals)\n",
    "for i,j in enumerate(eig_pair):\n",
    "    print('eigvalue{0:}:{1:.2%}'.format(i+1,(j[0]/eigv_sum).real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix W:\n",
      " [[-0.2049 -0.009 ]\n",
      " [-0.3871 -0.589 ]\n",
      " [ 0.5465  0.2543]\n",
      " [ 0.7138 -0.767 ]]\n"
     ]
    }
   ],
   "source": [
    "# 选择K =2 个特征向量作为映射矩阵，这里选了前2个有信息的。  \n",
    "W = np.hstack((eig_pair[0][1].reshape(4,1),eig_pair[1][1].reshape(4,1)))\n",
    "\n",
    "print('Matrix W:\\n', W.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 5:将样本投影到新空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 通过LDA降维，将数据降维为2维特征\n",
    "X_lda = X.dot(W)\n",
    "\n",
    "assert X_lda.shape == (150,2), 'The matrix is not 150*2 dimensional.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 可视化\n",
    "from matplotlib import pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
