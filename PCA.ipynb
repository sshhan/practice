{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 降维技术\n",
    "## 1.主成分分析-PCA-SVD\n",
    "    第一种降维的方法称为主成分分析(Principal Component Analysis, PCA)。    \n",
    "   在PCA中，数据从原来的坐标系换到了新的坐标系，新的坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择与第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。事实上，大部分方差都包含在最前面的几个坐标轴中。因此，可以舍弃余下的坐标轴，即对数据进行了降维处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 PCA 思想\n",
    "- PCA的思想是将d维特征映射到k维上$(k<d)$, 这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从d维特征中去除其余$d-k$维特征。  \n",
    "  \n",
    "令$y\\in R^k$是由特征向量$x\\in R^d$经过PCA降维后得到的新的特征向量，这里$(k<d)$。则有：  \n",
    "$$y=W^Tx\\qquad(1)$$\n",
    "其中，$W$是$d\\times k$维且正交的。\n",
    "> - W由x的统计特征决定\n",
    "> - 假设均值和协方差阵为：$E[x]=0,E[xx^T]=C_x$ \n",
    "\n",
    "### 1.2 最近重构性-PCA 推导\n",
    "- 先考虑一个问题：对于正交属性空间中的样本点，如何用一个超平面对所有的样本进行恰当的表达？\n",
    "> 最近重构性： 样本点到这个超平面的距离都足够近。\n",
    "\n",
    "根据最近重构性，我们得到主成分分析的推导过程。  \n",
    "\n",
    "由(1)式和PCA思想，我们假设样本进行了中心化处理(为什么)。  \n",
    "再假定进行投影变换后得到新的坐标系$(w_1,w_2,\\cdots,w_d)$,其中$w_i$是标准正交基向量，$w_i^Tw_i=1$，$w_i^Tw_j=0$。若丢弃新坐标系的部分坐标$W=(w_1,w_2,\\cdots,w_k)$，即将维度降为$(k<d)$。则样本在新坐标系的投影为y。  \n",
    "\n",
    "- 若基于y来重构x，则会得到 $x_r=Wy$. (W是正交阵，则有$W^T=W^{-1}$)  \n",
    "- 考虑原样本点与基于投影重构的样本点$x_r$之间的距离(也叫误差)为： \n",
    "\n",
    "$$\\epsilon=x-x_r=x-WW^Tx$$  \n",
    "- 距离平方为：  \n",
    "\\begin{eqnarray}\n",
    "\\lVert\\epsilon\\rVert^2&=&\\epsilon^T\\epsilon\\notag\\\\\n",
    "&=&(x-WW^Tx)^T(x-WW^Tx)\\notag\\\\\n",
    "&=&x^Tx-2x^TWW^Tx+x^TWW^TWW^TX\\notag\\\\\n",
    "&=&x^Tx-x^TWW^Tx\\notag\\\\\n",
    "\\end{eqnarray}\n",
    "> 注意：这里$W^TW=I$.  \n",
    "- 令$k=1$,即W是一个向量，y是标量。则有期望误差为：\n",
    "\\begin{eqnarray}\n",
    "E[\\epsilon^T\\epsilon]&=&E[x^Tx]-E[(x^Tw)(w^Tx)]\\notag\\\\\n",
    "&=&E[x^Tx]-E[w^Txx^Tw]\\notag\\\\\n",
    "&=&E[x^Tx]-w^TE[xx^T]w\\notag\\\\\n",
    "&=&E[x^Tx]-w^TC_xw\\notag\\\\\n",
    "\\end{eqnarray}\n",
    "- 由最近重构性，最小化$E[\\epsilon^T\\epsilon]$.等价于 \n",
    "$$\\max_w w^TC_xw$$  \n",
    "- 标准化w的长度，故定义：  \n",
    "$$ J=\\dfrac{w^TC_xw}{w^Tw}$$\n",
    "- 目标：找到使得J最大化的w  \n",
    "对J函数求导数并令其为0，得到：  \n",
    "$$\\dfrac{\\partial J}{\\partial w}=\\dfrac{(w^Tw)2C_xw-(w^TC_xw)2w}{(w^Tw)^2}=0$$\n",
    "$$0=\\dfrac{2C_xw}{w^Tw}-[\\dfrac{w^TC_xw}{w^Tw}]\\cdot\\dfrac{2w}{w^Tw}$$\n",
    "- 化简可得：  \n",
    "$$C_xw=Jw \\qquad \\leftarrow\\mathrm{Eigenvalue\\quad Problem} $$  \n",
    "\n",
    "\n",
    "- 由上可知，w是$C_x$的最大特征值J对应的特征向量。\n",
    "\n",
    "***\n",
    "#### 1.2.1 总结：  \n",
    "\n",
    "一般的PCA形式为： $y=W^T(x-m)$.   \n",
    "\n",
    "其中$m=E[x]$是均值向量，$W$是$d\\times k$维矩阵-包含$\\mathrm{Var}[x]$的前k个最大特征值对应的特征向量。$W=(w_1,\\cdots,w_k)$,这里$w_i$是正交向量.  \n",
    "- 其中，$w_1$：第一主成分；  \n",
    "- $w_2$: 第二主成分，等等。  \n",
    "> 1. PCA是对坐标轴的平移旋转变换；  \n",
    "> 2. $w_1$：最大延长方向；  \n",
    "> 3. $w_2$：第二大的延长方向，且正交于前一个特征向量；  \n",
    "> 4. W是正交的-因为$C_x$是对称矩阵 (对称阵有一个很优美的性质:它总能相似对角化,对称阵不同特征值对应的特征向量两两正交)； \n",
    "> 5. $WW^T\\neq I$除非$k=d$\n",
    "\n",
    "***\n",
    "#### 1.2.2 PCA计算  \n",
    "1. 随机变量x的最佳压缩是在最小均方差意义下的，由PCA的导出所保证；  \n",
    "2. 去相关性：$y=W^T(x-m)$  \n",
    "$\\mathrm{Var}[y]=C_y=W^TC_xW=\\Lambda$是对角特征值矩阵，即，$y=(y_1,\\cdots,y_k)^T$的元素之间是不相关的。(实对称阵可以正交相似对角化). \n",
    "\n",
    "\n",
    "1. 思考：怎么选k-重构阈值\n",
    "choose k so that ratio $\\dfrac{\\sum_{i=1}^k\\lambda_i}{\\sum_{j=1}^d\\lambda_j}>90\\%$  \n",
    "2. 计算：$C_x,m$. \n",
    "给定样本数据$x_1,\\cdots,x_N$估计$C_x,m$.  \n",
    "\n",
    "\n",
    "样本均值为$$\\hat{m}=\\dfrac{1}{N}\\sum_{i=1}^N x_i$$ \n",
    "样本协方差矩阵：  \n",
    "$$\\hat{C}_x=\\dfrac{1}{N}\\sum_{i=1}^N(x_i-m)(x_i-m)^T$$\n",
    "$$\\mathrm{or}\\dfrac{1}{N-1}\\sum_{i=1}^N(x_i-m)(x_i-m)^T$$  \n",
    "或者散步矩阵$$S=\\sum_{i=1}^N(x_i-m)(x_i-m)^T$$  \n",
    "一般来说，若$N<<d$,则$\\hat{C}_x$是不满秩的，$\\hat{C}_x$也会非常大，$d\\times d$维的矩阵。  \n",
    "计算技巧：使用内积！\n",
    "令：  \n",
    "$$A=\\left[\\begin{matrix}\n",
    "|&|& &|\\\\\n",
    "x_1-m&x_2-m&\\cdots&x_N-m\\\\\n",
    "|&|& &|\n",
    "\\end{matrix}\\right]\n",
    "$$  \n",
    "A是$d\\times N$维矩阵，则$A^TA$是$N\\times N$维矩阵(远小于$d\\times d$)。  \n",
    "寻找$A^TA$的特征值和特征向量，则有：  \n",
    "$$\\Longrightarrow  A^TAv=\\lambda v$$  \n",
    "$$ (AA^T)Av=\\lambda Av$$\n",
    "$$\\Longrightarrow Av \\mathrm{\\quad is\\quad eigenvector\\quad of\\quad} AA^T$$  \n",
    "注意到：$AA^T=\\sum_{i=1}^N(x_i-m)(x_i-m)^T=S$,是散步矩阵，这样我们可以避免计算$AA^T$.\n",
    "***\n",
    "#### 1.2.3 补充：复习均值与方差\n",
    "随机变量x的期望为：\n",
    "$$E[x]=\\int x\\mathrm{p}(x)dx$$  \n",
    "记$E[x]=m$,则随机变量x的方差为：  \n",
    "\n",
    "$$\\mathrm{Var}[x]=E[(x-m)^2]=E[x^2]-m^2$$  \n",
    "\n",
    "对于向量$x=(x_1,\\cdots,x_d)^T$而言，期望为：  \n",
    "\n",
    "$$m=E[x]=[E[x_1],E[x_2],\\cdots,E[x_d]]^T$$  \n",
    "协方差矩阵为：  \n",
    "$$\\mathrm{Var}[x]=E[(x-m)(x-m)^T]=E[xx^T]-mm^T$$ \n",
    "注意： 协方差矩阵是对称且半正定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 PCA 理论基础\n",
    "#### 1.3.1 最大方差理论\n",
    "在信号处理中认为信号具有较大的方差，噪声具有较小的方差，信噪比就说信号与噪声低方差比，越大越好。  \n",
    "统计学中，方差是指数据分布的大致差幅。一个具有较大方差的特征它的样本散布的数值范围极大，而方差较小则特征的样本分布通常是紧密聚集在一起的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 因子分析-Factor Analysis\n",
    "第二种降维技术是因子分析(Factor Analysis)。在因子分析中，我们假设在观察数据的生成中有一些观察不到的隐变量。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目少，也就是说通过找到隐变量就可以实现数据的降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 独立成分分析-ICA\n",
    "第三种降维技术就是独立成分分析(Independent Component Analysis, ICA)。ICA假设数据是从N个数据源上生成的，这一点和因子分析相似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中只假设数据是不相关的。同因子分析一样，如果数据源的树木少于观察数据的数目，则可以实现降维过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么要使用协方差矩阵？\n",
    "#### 1. 关于投影矩阵／变换矩阵(为什么投影阵是正交矩阵 )\n",
    "\n",
    "一个向量经过变换矩阵，只要变换矩阵的基分量之间是相互线性无关的就可以进行转换。只要满足“线性无关的要求”，数据就可以从一个度量空间转换到另一个空间,而且可以相互转换。  \n",
    "例如$b=Ax,x=A^{-1}b$，转换矩阵为A(这里假定A是可逆阵)。  \n",
    "对A进一步加上正交矩阵的要求，就可以得到：$AA^T=I\\rightarrow A^T=A^{-1}$。\n",
    "> - 在求解坐标转换时速度更快，$x=A^Tb$,因为求转置矩阵比矩阵求逆速度快很多。\n",
    "- 基向量两两正交，说明两个向量相关性很小，相当于数据在这两个维度上的相关性很小。线性无关不等于不相关。  \n",
    "\n",
    "\n",
    "#### 2. 协方差矩阵\n",
    "\n",
    "在降维的过程中我们希望去除的信息：  \n",
    "- 噪声：降噪的目的就是使保留下来的特征间的相关性尽可能小。\n",
    "- 冗余：去冗余的目的就是使保留下来的特征含有的方差尽可能大。  \n",
    "\n",
    "协方差矩阵度量的是维度与维度(特征与特征)之间的关系，而非样本与样本之间。协方差矩阵的主对角线上的元素是各个特征上的方差，其他元素是两两特征之间的协方差-相关性。   \n",
    "显然，协方差度量可以满足上面的两种要求。使协方差矩阵中非对角线元素基本为0来实现\"降噪\"，对角线上元素尽可能的大来实现“去冗余”。  \n",
    "> 为了实现这个目的，我们对协方差矩阵进行对角化。  \n",
    "对角化后得到的矩阵，其对角线上是协方差矩阵的特征值。此外，它还是各个维度上的新方差。对角线上的较小的新方差就是那些该舍弃的特征。 \n",
    "\n",
    "#### 3.正交变换的好处\n",
    "定义 若P为正交矩阵，则线性变换$y=Px$为正交变换。  \n",
    "设$y=Px$为正交变换，则有：  \n",
    "$$\\lVert y\\rVert=\\sqrt{y^Ty}=\\sqrt{x^TP^TPx}=\\sqrt{x^Tx}=\\lVert x\\rVert$$  \n",
    "说明经正交变换线段长度保持不变。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
